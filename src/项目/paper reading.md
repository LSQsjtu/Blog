---
layout: post
title: Paper Reading
slug: Paper Reading
date: 2022-04-15
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - paper
excerpt: Mu Li系列
---

**阅读论文**

1.title
2.abstract
3.introduction
4.method
5.experiments
6.conclusion



- 第一遍：==标题、摘要、结论==。可以看一看==方法和实验部分重要的图和表==。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。
- 第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要==了解重要的图和表==，知道每一个部分在干什么，作者相比与其他人提出了什么==不同的方法==，圈出==相关文献==。觉得文章太难，可以读==引用的文献==。
- 第三遍：提出什么==问题==，用什么==方法==来解决这个问题。==实验==是怎么做的。合上文章，回忆每一个部分在讲什么。换我来可以怎么做，==能不能做得更好==。



论文项目三步分解法

1. 问题
2. 方法
3. 结果，以及未来畅想



**经典论文**

==Resnet==

残差连接：学习浅层网络没有学习到的residual，当输入为0时后面的网络层基本为0，没有多余的信息，学习==$H(x) - x$==的信息。网络结构的设计使得梯度稳定，不会到零（后人的解释）。原始想法是将机器学习中学习残差的思想运用到深度网络训练中。

bottleneck：先降维，然后提取信息，再提升维度，起到降低算法复杂度的功能，不用学习无用维度的信息。



==Generative adversarial nets==

写作方面

- 对于开创性的工作来说主要说明自己独一无二的地方，基于其他方法改进的主要突出强调区别



创新点

- 以往是近似构造似然函数去学习这个分布，现在通过模型网络来直接**学习分布**（于是可以训练了），分布的求解往往是不可逆的，无法求出分布的逆函数，来通过参数求出对应的输出数据，于是采用非常大的数据去盗版模仿
- 通过G和D去对抗，minmax最优化分布函数，求取最优值



新的知识点

- ==KL散度==：如果D过拟合会导致输出为0，不好优化求导，后面有工作去更新优化方式，通过更简约的方法去学习这个分布。
- 如果数据不属于同一个分布，那么就可以训练同一个二分类去分类