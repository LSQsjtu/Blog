---
layout: post
title: Paper Reading
slug: Paper Reading
date: 2022-04-15
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - paper
excerpt: Mu Li系列
---

**阅读论文**

1.title
2.abstract
3.introduction
4.method
5.experiments
6.conclusion



- 第一遍：==标题、摘要、结论==。可以看一看==方法和实验部分重要的图和表==。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。
- 第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要==了解重要的图和表==，知道每一个部分在干什么，作者相比与其他人提出了什么==不同的方法==，圈出==相关文献==。觉得文章太难，可以读==引用的文献==。
- 第三遍：提出什么==问题==，用什么==方法==来解决这个问题。==实验==是怎么做的。合上文章，回忆每一个部分在讲什么。换我来可以怎么做，==能不能做得更好==。



论文项目三步分解法

1. 问题
2. 方法
3. 结果，以及未来畅想



**经典论文**

==Resnet==

残差连接：学习浅层网络没有学习到的residual，当输入为0时后面的网络层基本为0，没有多余的信息，学习==$H(x) - x$==的信息。网络结构的设计使得梯度稳定，不会到零（后人的解释）。原始想法是将机器学习中学习残差的思想运用到深度网络训练中。

bottleneck：先降维，然后提取信息，再提升维度，起到降低算法复杂度的功能，不用学习无用维度的信息。



==Generative adversarial nets==

写作方面

- 对于开创性的工作来说主要说明自己独一无二的地方，基于其他方法改进的主要突出强调区别



创新点

- 以往是近似构造似然函数去学习这个分布，现在通过模型网络来直接**学习分布**（于是可以训练了），分布的求解往往是不可逆的，无法求出分布的逆函数，来通过参数求出对应的输出数据，于是采用非常大的数据去盗版模仿
- 通过G和D去对抗，minmax最优化分布函数，求取最优值



新的知识点

- ==KL散度==：如果D过拟合会导致输出为0，不好优化求导，后面有工作去更新优化方式，通过更简约的方法去学习这个分布。
- 如果数据不属于同一个分布，那么就可以训练同一个二分类去分类



**BERT**

芝麻街中人物姓名

解决问题

- 可以预训练后广泛运用在其他任务上（加一个输出层）
- 将预训练过程和大模型过程



创新点

- 引入==mask==，完形填空，从而可以利用双向的信息，而不是单纯地就将从左向右和从右向左结合在一起
- 证明了利用大量没有标号的数据训练，比利用少量有标号的数据训练要更好
- 任务相对简单，故



写作

- 数值展示：绝对值（相对值）
- 比较，开门见山，比较BERT和GPT，Elmo

> GPT（在下游任务上微调，分阶段训练）
>
> ELmo: RNN 提取特征后再一起用于训练



**MAE: masked autoencoder**

解决问题：将Bert引入cv中存在三个问题

1.  位置信息不好和图片信息一起输入transform中（但是一般CNN网络架构中顺序卷积会将位置信息传递下去），==ViT已经解决==。
2. 信息密度不一样（词和图片），词不好去掉，但是图片冗余信息多，所以要丢掉高比率随机块（去掉很多块）、
3. decoder的设计，不同层次信息



创新点：

- 将ViT和Bert结合起来，构建非对称的autoencoder（encoder只关注没有mask的部分，decoder关注所有信息，并且输入对应的位置信息，重新==生成==mask的部分）
- 发现盖住75%的图片，random掩码能够得到较好的信息（示例图片amazing，但是对于其他图片或许没有这么好的效果）



知识点：

- ViT：将图片分割成16$\times$16的分块，当做词语来学习
- auto：x，y来自同一个样本（如同一个句子或同样的图片），快的模型：efficient，大的模型：scalable
- 字词是高层次语义信息，所以简单输出层encoder就可以了，图片语义分割常采用转置的神经网络解码器



写作：

> 名字：**什么是什么**
>
> 写清训练和测试过程中的操作安排，讲好自己为什么这样做，并且有数据支撑