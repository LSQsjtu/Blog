---
layout: post
title: pytorch基础教程
slug: base pytorch
date: 2021-10-26
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - 深度学习
excerpt: 基础理论
---

**pytorch基础教程**

tensors是一种特殊的数据类型和arrays，matrices类似。用tensors编码代替input和output，还有模型参数。

# 创建初始化方式

1. 通过data生成torch.tensor(data)，这时类型自动推断，和data中类型一致

2. 从numpy array

```python
np_array = np.array(data)
x_np = torch.from_numpy(np_array)
```

3. 从另一个tensor

   隐式地保持类型一致

```python
x_ones = torch.ones_like(x_data) # retains the properties of x_data
print(f"Ones Tensor: \n {x_ones} \n")

x_rand = torch.rand_like(x_data, dtype=torch.float) # overrides the datatype of x_data
print(f"Random Tensor: \n {x_rand} \n")
```

4. 调用函数

shape要是tensor维度的tuple

```python
shape = (2, 3,)
rand_tensor = torch.rand(shape)
ones_tensor = torch.ones(shape)
zeros_tensor = torch.zeros(shape)

print(f"Random Tensor: \n {rand_tensor} \n")
print(f"Ones Tensor: \n {ones_tensor} \n")
print(f"Zeros Tensor: \n {zeros_tensor}")
```

# tensor属性

tensor.shape,tensor.dtype（datatype的缩写）,torch.device

计算时，类型对齐的便捷方式为`x0.type_as(x)-x`

# tensor数学运算

transposing, indexing, slicing, mathematical operations, linear algebra, random sampling, and more are comprehensively described [here](https://pytorch.org/docs/stable/torch.html).

tensor.to(‘cuda’)还可以显式在定义时指定device=torch.device(“cuda”)

1. 和numpy一样的切片操作

2. 用torch.cat拼接tensor（torch.stack用的较少），torch.cat可以指定维度

3. 乘法，对应位置相乘

```python
# This computes the element-wise product
print(f"tensor.mul(tensor) \n {tensor.mul(tensor)} \n")
# Alternative syntax:
print(f"tensor * tensor \n {tensor * tensor}")
```

4. 矩阵乘法

```python
print(f"tensor.matmul(tensor.T) \n {tensor.matmul(tensor.T)} \n")
# Alternative syntax:
print(f"tensor @ tensor.T \n {tensor @ tensor.T}")
```

5. 替代修改操作

Operations that have a `_` suffix are in-place. For example: `x.copy_(y)`, `x.t_()`, will change `x`.

```python
print(tensor, "\n")
tensor.add_(5)
print(tensor)

out:
tensor([[1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.],
        [1., 0., 1., 1.]])

tensor([[6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.],
        [6., 5., 6., 6.]])
```

in_place操作会节省内存，但是求导时会报错，故不鼓励使用

6. torch.expand()

-1默认不该变这一维的大小，其他改变的相当于将这一维复制扩展

7. torch.gather()

> torch.gather(*input*, *dim*, *index*, ***, *sparse_grad=False*, *out=None*)
>
> ```python
> out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
> out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
> out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
> ```

将dim索引改成对应的index\[i]\[j]\[k]，**从完整数据中按索引取值**

## Bridge with NumPy

```python
t = torch.ones(5)
print(f"t: {t}")
n = t.numpy()
print(f"n: {n}")
t.add_(1)
print(f"t: {t}")
print(f"n: {n}")

out：
t: tensor([1., 1., 1., 1., 1.])
n: [1. 1. 1. 1. 1.]
t: tensor([2., 2., 2., 2., 2.])
n: [2. 2. 2. 2. 2.]
```

两者会同时修改，潜在的空间一样

# `TORCH.AUTOGRAD`

**Forward Propagation**: 产生最佳最正确的输出

**Backward Propagation**: 调整它的参数根据生成的error

i.e.:**abbr.**亦即（源自拉丁文 id est）

w.r.t. :with respect to，相对某一方面而言

backward后会把loss值free，如果loss不是标量，需要指定gradient

```python
a = torch.tensor([2., 3.], requires_grad=True)
b = torch.tensor([6., 4.], requires_grad=True)

Q = 3*a**3 - b**2

external_grad = torch.tensor([1., 1.])
Q.backward(gradient=external_grad)
```

directed acyclic graph (DAG)有向无环图，leaves are the input tensors, roots are the output tensors.倒过来的树，从root跟踪到leaves自动计算梯度

**frozen parameters**：用来微调模型参数

# NEURAL NETWORKS

- Define the neural network that has some learnable parameters (or weights)
- Iterate over a dataset of inputs
- Process input through the network
- Compute the loss (how far is the output from being correct)
- Propagate gradients back into the network’s parameters
- Update the weights of the network, typically using a simple update rule: `weight = weight - learning_rate * gradient`



`torch.Tensor` - A *multi-dimensional array* with support for autograd operations like `backward()`.

`nn.Module`-封装好的类，方便move到GPU上



```python
print(loss.grad_fn)  # MSELoss
print(loss.grad_fn.next_functions[0][0])  # Linear
print(loss.grad_fn.next_functions[0][0].next_functions[0][0])  # ReLU
```

这样可以观察计算图

每次backward前，把计算图梯度清零，否则每次会积累

# 数据处理

首先导入为ndarray，然后转换为torch.*Tensor