---
layout: post
title: pytorch代码可重复性
slug: 可重复性总结
date: 2021-7-18
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - 深度学习
excerpt: 3天训练代码阅读的思考
---

**训练的可重复性**

1. **CUDNN**
cudnn中对卷积操作进行了优化，牺牲了精度来换取计算效率。如果需要保证可重复性，可以使用如下设置:

```python
from torch.backends import cudnn
cudnn.benchmark = False            # if benchmark=True, deterministic will be False
cudnn.deterministic = True
```

不过实际上这个设置对精度影响不大，仅仅是小数点后几位的差别。所以如果不是对精度要求极高，其实不太建议修改，因为会使计算效率降低。

 

2. **Pytorch**

  ```python
  torch.manual_seed(seed)            # 为CPU设置随机种子
  torch.cuda.manual_seed(seed)       # 为当前GPU设置随机种子
  torch.cuda.manual_seed_all(seed)   # 为所有GPU设置随机种子
  ```

  

3. **Python & Numpy**
如果读取数据的过程采用了随机预处理(如RandomCrop、RandomHorizontalFlip等)，那么对python、numpy的随机数生成器也需要设置种子。

```python
import random
import numpy as np
random.seed(seed)
np.random.seed(seed)
```




最后，关于**dataloader**：

注意，如果dataloader采用了多线程(num_workers > 1), 那么由于读取数据的顺序不同，最终运行结果也会有差异。也就是说，改变num_workers参数，也会对实验结果产生影响。目前暂时没有发现解决这个问题的方法，但是只要固定num_workers数目（线程数）不变，基本上也能够重复实验结果。

对于不同线程的随机数种子设置，主要通过DataLoader的worker_init_fn参数来实现。默认情况下使用线程ID作为随机数种子。如果需要自己设定，可以参考以下代码：

```python
GLOBAL_SEED = 1

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

GLOBAL_WORKER_ID = None
def worker_init_fn(worker_id):
    global GLOBAL_WORKER_ID
    GLOBAL_WORKER_ID = worker_id
    set_seed(GLOBAL_SEED + worker_id)

dataloader = DataLoader(dataset, batch_size=16, shuffle=True, num_workers=2, worker_init_fn=worker_init_fn)
```

