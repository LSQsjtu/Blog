---
layout: post
title: 20号奇怪想法
slug: strange idea
date: 2021-7-20
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - 深度学习
excerpt: 综述之后的总结
---

**七月二十日**

KD蒸馏的新奇地方

1. response-base knowledge：用最后的logits，最简单直接
2. feature-base knowledge：loss方法，hint layer的选取没有明确的指示，让中间层的特征，attention map一致
3. relation-base knowledge：同时使用logits和中间层的feature

训练方法，offline，online，self-distillation

stu-network设计暂时不设计

训练算法：使用gan，生成数据，增强数据集

多种模型压缩算法使用增强网络功能

小的教师网络增强大的学生网络的学习



feature-base knowledge distillation开源文章及代码：

**Cross-Layer Distillation with Semantic Calibration**（[GitHub - DefangChen/SemCKD: This is the official implementation for the AAAI-2021 paper (Cross-Layer Distillation with Semantic Calibration).](https://github.com/DefangChen/SemCKD)  AAAI被引用次数1

2014-2021KD论文整理：[FLHonker/Awesome-Knowledge-Distillation: Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)](https://github.com/FLHonker/Awesome-Knowledge-Distillation)

attetionmap use neuron selectivity transfer
