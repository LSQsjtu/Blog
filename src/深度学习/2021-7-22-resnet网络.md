---
layout: post
title: resnet网络
slug: resnet
date: 2021-7-22
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - 深度学习
excerpt: 残差网络学习
---

**restnet**

1. **简化了学习过程，增强了梯度传播**

相比于学习原始的信号，残差网络学习的是信号的差值，这在许多的研究中被验证是更加有效的，它简化了学习的过程。

**根据我们前面的内容可知，在一定程度上，网络越深表达能力越强，性能越好。**

然而随着网络深度的增加，带来了许多优化相关的问题，比如梯度消散，梯度爆炸。

残差网络从根本上解决了梯度问题

![](https://res-static.hc-cdn.cn/fms/img/fb9d7c7db3bb265f7f997160aa48bc641603780769781)

2. **打破了网络的不对称性**

虽然残差网络可以通过跳层连接，增强了梯度的流动，从而使得上千层网络的训练成为可能，**不过相关的研究表面残差网络的有效性，更加体现在减轻了神经网络的退化。**

**如果在网络中每个层只有少量的隐藏单元对不同的输入改变它们的激活值，而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低，这就是我们常说的网络退化问题。**

3. **增强了网络的泛化能力**





一个残差块可以用表示为：

![](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D%3D+x_l%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B1%7D)

残差块分成两部分直接映射部分和残差部分。 ![[公式]](https://www.zhihu.com/equation?tex=h%28x_l%29) 是直接映射，反应在图1中是左边的曲线； ![[公式]](https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29) 是残差部分，一般由两个或者三个卷积操作构成，即下图中右侧包含卷积的部分。

![img](https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg)

上图中的Weight在卷积网络中是指卷积操作，addition是指单位加操作。

**残差网络的背后原理**

残差块一个更通用的表示方式是

![[公式]](https://www.zhihu.com/equation?tex=y_l%3D+h%28x_l%29%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B3%7D)

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+f%28y_l%29%5Ctag%7B4%7D)

现在我们先不考虑升维或者降维的情况，那么在[1]中， ![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ccdot%29) 是直接映射， ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ccdot%29) 是激活函数，一般使用ReLU。我们首先给出两个假设：

- 假设1： ![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ccdot%29) 是直接映射；
- 假设2： ![[公式]](https://www.zhihu.com/equation?tex=f%28%5Ccdot%29) 是直接映射。

那么这时候残差块可以表示为：

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+x_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B5%7D)

对于一个更深的层 ![[公式]](https://www.zhihu.com/equation?tex=L) ，其与 ![[公式]](https://www.zhihu.com/equation?tex=l) 层的关系可以表示为

![[公式]](https://www.zhihu.com/equation?tex=x_L+%3D+x_l+%2B+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%5Ctag%7B6%7D)

这个公式反应了残差网络的两个属性：

1. ![[公式]](https://www.zhihu.com/equation?tex=L) 层可以表示为==任意一个比它浅的l层和他们之间的残差部分之和；==
2. ![[公式]](https://www.zhihu.com/equation?tex=x_L%3D+x_0+%2B+%5Csum_%7Bi%3D0%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29) ， ![[公式]](https://www.zhihu.com/equation?tex=L) 是各个残差块特征的单位累和，而MLP是特征矩阵的累积。

根据BP（back propagation）中使用的导数的链式法则，损失函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon) 关于 ![[公式]](https://www.zhihu.com/equation?tex=x_l) 的梯度可以表示为

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%5Cfrac%7B%5Cpartial+x_L%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%281%2B%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%29+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%2B%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29+%5Ctag%7B7%7D)

上面公式反映了残差网络的两个属性：

1. 在整个训练过程中， ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29+) 不可能一直为 ![[公式]](https://www.zhihu.com/equation?tex=-1) ，也就是说在残差网络中==不会出现梯度消失的问题==。
2. ![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D) 表示 ![[公式]](https://www.zhihu.com/equation?tex=L) 层的梯度可以直接传递到任何一个比它浅的 ![[公式]](https://www.zhihu.com/equation?tex=l) 层。

通过分析残差网络的正向和反向两个过程，我们发现，当残差块满足上面两个假设时，信息可以非常畅通的在高层和低层之间相互传导，说明这两个假设是让残差网络可以训练深度模型的充分条件。那么这两个假设是必要条件吗？

**直接映射是最好的选择**

对于假设1，我们采用反证法，假设 ![[公式]](https://www.zhihu.com/equation?tex=h%28x_l%29+%3D+%5Clambda_l+x_l) ，那么这时候，残差块（图3.b）表示为

![[公式]](https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+%5Clambda_lx_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B8%7D)

对于更深的L层

![](https://www.zhihu.com/equation?tex=x_%7BL%7D+%3D+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_i%29x_l+%2B+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%5Ctag%7B9%7D)

为了简化问题，我们只考虑公式的左半部分 ![](https://www.zhihu.com/equation?tex=x_%7BL%7D+%3D+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_l%29x_l) ，损失函数 ![[公式]](https://www.zhihu.com/equation?tex=%5Cvarepsilon) 对 ![[公式]](https://www.zhihu.com/equation?tex=x_l) 求偏微分得

![](https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5Cvarepsilon%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D+%5Cleft%28+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_i%29+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_l%7D+%5Chat%7B%5Cmathcal%7BF%7D%7D%28x_i%2C+%5Cmathcal%7BW%7D_i%29%5Cright%29%5Ctag%7B10%7D+)

上面公式反映了两个属性：

1. 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3E1) 时，很有可能发生梯度爆炸；
2. 当 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda%3C1) 时，梯度变成0，会阻碍残差网络信息的反向传递，从而影响残差网络的训练。

所以 ![[公式]](https://www.zhihu.com/equation?tex=%5Clambda) ==必须等1==。同理，其他常见的激活函数都会产生和上面的例子类似的阻碍信息反向传播的问题。

对于其它不影响梯度的 ![[公式]](https://www.zhihu.com/equation?tex=h%28%5Ccdot%29) ，例如LSTM中的门机制（图3.c，图3.d）或者Dropout（图3.f）以及[1]中用于降维的 ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积（图3.e）也许会有效果，作者采用了实验的方法进行验证，实验结果见图4

![img](https://pic2.zhimg.com/80/v2-843326b572e2e4c5c8956e289bd3f58d_720w.jpg)

图3：直接映射的变异模型

![img](https://pic4.zhimg.com/80/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_720w.jpg)图4：变异模型（均为110层）在Cifar10数据集上的表现

从图4的实验结果中我们可以看出，在所有的变异模型中，依旧是==直接映射的效果最好==。下面我们对图3中的各种变异模型的分析

1. Exclusive Gating：在LSTM的门机制中，绝大多数门的值为0或者1，几乎很难落到0.5附近。当 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow0) 时，残差块变成只有直接映射组成，阻碍卷积部分特征的传播；当 ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow1) 时，直接映射失效，退化为普通的卷积网络；
2. Short-cut only gating： ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow0) 时，此时网络便是[1]提出的直接映射的残差网络； ![[公式]](https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow1) 时，退化为普通卷积网络；
3. Dropout：类似于将直接映射乘以 ![[公式]](https://www.zhihu.com/equation?tex=1-p) ，所以会影响梯度的反向传播；
4. ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) conv： ![[公式]](https://www.zhihu.com/equation?tex=1%5Ctimes1) 卷积比直接映射拥有更强的表示能力，但是实验效果却不如直接映射，说明该问题更可能是优化问题而非模型容量问题。

所以我们可以得出结论：假设1成立，即

![[公式]](https://www.zhihu.com/equation?tex=y_l+%3D+x_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+w_l%29+%5Ctag%7B11%7D)

![[公式]](https://www.zhihu.com/equation?tex=y_%7Bl%2B1%7D+%3D+x_%7Bl%2B1%7D+%2B+%5Cmathcal%7BF%7D%28x_%7Bl%2B1%7D%2C+w_%7Bl%2B1%7D%29+%3D+f%28y_l%29+%2B+%5Cmathcal%7BF%7D%28f%28y_l%29%2C+w_%7Bl%2B1%7D%29+%5Ctag%7B12%7D)