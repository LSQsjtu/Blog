---
layout: post
title: 训练模式
slug: model.train
date: 2021-7-18
status: publish
author: 刘胜琪
categories: 
  - 默认分类
tags: 
  - 深度学习
excerpt: 自己需要提升基础代码知识
---

**model.train()和model.eval()**

model.train()

- **启用Batch Normalization 和 Dropout**

  model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。照设定的参数p设置保留激活单元的概率（保留概率=p)

model.eval()

- ​	**不启用 Batch Normalization 和 Dropout**

  model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元

  dropout层会让*所有的激活单元都通过*，而BN层会停止计算和更新mean和var，直接使用在训练阶段已经学出的*mean和var值*

- 在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播（back probagation)。



**model.eval()和with torch.no_grad()的区别**

with torch.no_grad()则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用。它的作用是将该with语句包裹起来的部分停止梯度的更新，从而节省了GPU算力和显存，但是并不会影响dropout和BN层的行为

而model.eval()在不更新梯度的情况下会影响dropout和BN层的情况



以后调用model时提前声明模式