<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><id>/Blog/</id><title>我的个人博客</title><updated>2022-07-18T02:38:00.724018+08:06</updated><author><name>LSQ</name><email>1959376918@qq.com</email></author><link href="/Blog/" rel="alternate"/><generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator><logo>https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/f-logo.png</logo><subtitle>记录生活美好</subtitle><entry><id>/Blog/archives/PR%E5%AD%A6%E4%B9%A0/</id><title>PR学习</title><updated>2022-07-18T02:38:00.724479+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;PR 教程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;导出设置，调整目标比特率&lt;/p&gt;&lt;p&gt;![image-20220524220508293](PR 教程.assets/image-20220524220508293-16534011104501.png)&lt;/p&gt;&lt;p&gt;字幕是引用，所以每次都新建一个新的字幕，选择New Time Romin字体，居中……&lt;/p&gt;&lt;p&gt;视频直接拖动，音频拖动需要先解除链接，每个视频在不同位置，调整缩放是不需要选择帧这一项&lt;/p&gt;&lt;p&gt;图片可以直接拉动改变时长&lt;/p&gt;&lt;p&gt;![image-20220524223821001](PR 教程.assets/image-20220524223821001-16534031026721.png)&lt;/p&gt;</content><link href="/Blog/archives/PR%E5%AD%A6%E4%B9%A0/" rel="alternate"/><published>2022-05-24T00:00:00+08:06</published></entry><entry><id>/Blog/archives/shining/</id><title>shining</title><updated>2022-07-18T02:38:00.724447+00:00</updated><author><name>lsq</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;闪灵&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;建立在印第安人坟地上的诅咒。当人自我走向罪恶的时候，不顾一切地去摧毁阻碍的东西，通过bgm渲染恐怖的氛围，将幽闭恐惧和超自然力量shining结合在一起，并不会让人感觉非常的害怕惊悚，没有突入其来的虐杀和恐怖，更多是通过演员的表现力传递出令人压迫的恐惧。&lt;/p&gt;&lt;p&gt;镜头调度非常好，但是由于打光的影响导致酒店所有房间都打开了灯，无法营造黑暗局促，压抑的氛围。通过揣度人物内心的黑暗面，细思极恐。&lt;/p&gt;</content><link href="/Blog/archives/shining/" rel="alternate"/><published>2022-06-11T00:00:00+08:06</published></entry><entry><id>/Blog/archives/%E4%BF%9D%E7%A0%94%E5%87%86%E5%A4%87/</id><title>保研准备</title><updated>2022-07-18T02:38:00.724407+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;ul&gt;
&lt;li&gt;[ ] 熟悉c++&lt;/li&gt;
&lt;li&gt;[ ] 力扣刷题&lt;/li&gt;
&lt;li&gt;[ ] 英语自我介绍&lt;/li&gt;
&lt;li&gt;[ ] 论文概括，英文视频&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;老师关注的是安卓开发，MMMOT框架跟踪&lt;/p&gt;&lt;p&gt;线段树修改分为单点修改和区间修改：&lt;/p&gt;&lt;p&gt;区间修改使用到lazy标志，当需要查询节点下的值，需要将lazy更新到子树中，lazy=INF表示已经更新过了。&lt;/p&gt;</content><link href="/Blog/archives/%E4%BF%9D%E7%A0%94%E5%87%86%E5%A4%87/" rel="alternate"/><published>2022-06-14T00:00:00+08:06</published></entry><entry><id>/Blog/archives/kernel%20approximation/</id><title>kernel approximation</title><updated>2022-07-18T02:38:00.724370+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;kernel approximation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;用random tensor逼近kernel matrix&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;如何使用random feature去训练数据&lt;/li&gt;
&lt;li&gt;多少random feature需要去确保质量&lt;/li&gt;
&lt;li&gt;evaluation of popular random features based algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;data dependent（根据数据distribution采样）和data-independent（高斯分布，高斯采样就可以）&lt;/p&gt;&lt;p&gt;sample number bigger than dim but much smaller than n.&lt;/p&gt;&lt;p&gt;random feature model可以看作两层神经网络（固定参数）&lt;/p&gt;&lt;figure style="flex: 123.66071428571429" &gt;&lt;img width="554" height="224" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/0a5eb3adf2a8b1653a61a19b83727cf9.jpg" /&gt;&lt;figcaption&gt;double descent phenomena&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其中左边的部分即为传统统计学习中的bias-variance trade-off，此时模型的参数小于interpolation threshold，实际上这个threshold即为模型数与样本数相等的点（在多分类的情况下略有变化）。当模型的参数超过这个threshold的时候，模型通过进一步增加参数以增加可能的最小范数模型的范围、进而达到降低test error。考虑到double descent现象是从最近的统计学习方法中发现的，与我们经典统计学习中的“U型”特征不同，故称上图左侧为classical regime，此时模型的状态是under-parameterized；而称上图右侧为modern interpolating regime，此时模型的状态是over-parameterized。&lt;/p&gt;&lt;p&gt;kernel learning by random features是通过学习分布得到。先学习random feature&lt;/p&gt;&lt;p&gt;通过算法可以得到random feature数量级单位&lt;/p&gt;</content><link href="/Blog/archives/kernel%20approximation/" rel="alternate"/><published>2022-06-18T00:00:00+08:06</published></entry><entry><id>/Blog/archives/neural%20tangent%20kernel%20%28NTK%29/</id><title>neural tangent kernel (NTK)</title><updated>2022-07-18T02:38:00.724329+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;h3&gt;neural tangent kernel (NTK)&lt;/h3&gt;
&lt;h4&gt;观点：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;NTK在无限宽极限下趋于一个确定的核(Kernel)，在无限宽极限下不随着时间变化==（是训练中的不变量）==lazy training，but not always the case，说明在参数空间中变化很小，所以可以用一阶泰勒近似，只是linear in w，对w求导，not linear in x（有非线性激活函数）。而且在梯度下降的训练过程中保持不变，因此==无限宽网络==的输出层结果的动力学可以用一个常微分方程来表示。可以控制梯度，有上下限。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;无限宽的神经网络等价于高斯过程，相当于一个线性过程，可以线性叠加并且可以在不同初始条件收敛到同一过程&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean field Theory中存在超参数平面中的一条临界线&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;figure style="flex: 195.3804347826087" &gt;&lt;img width="719" height="184" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/3dd0f1c10fc0e2b9c38a15962b662f01.png" /&gt;&lt;figcaption&gt;image-20220620212719766&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;==引入NTK parameterziation==，是因为LeCun参数化会导致NTK在无限宽极限下发散。神经网络是不凸的，有很多局部最优点，通过NTK可以通过梯度下降到全局最优点&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220623165503742.png" alt="image-20220623165503742" style="zoom:50%;" /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK就是前h层梯度积，经验NTK在宽度趋向无穷时可以趋近理想NTK，NTK只和网络层数有关，网络层数&amp;gt;=2  ，这种无穷NTK比多项式更好&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK是低维的，低维的特性（数据），skip connection&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;提出的问题困难：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;NTK在有限宽网络会失效：&lt;strong&gt;有限宽度到无穷的NTK演变问题&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;kernel trick：kernel将低维映射到高维，容易造成数据复杂度上升，难以采样，但常常是不考虑值而是考虑两个输出的内积。用kernel matrix去描述，对称阵，且是半正定的，这样可以节省空间和计算开销&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;数学知识：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220622163321364.png" alt="image-20220622163321364" style="zoom:67%;" /&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220627225110376.png" alt="关键公式" style="zoom:50%;" /&gt;&lt;/p&gt;&lt;p&gt;其中第一个等式来自第二个性质，第二个等式来自第一个性质，而 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5CTheta_%5Cinfty%28X%2CX%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 就是那位deterministic的Kerenl。&lt;/p&gt;&lt;p&gt;于是网络输出函数的动力学方程变为：&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_t%28%5Ctheta%2C+x%29%7D%7B%5Cpartial+t%7D+%3D++-+%5Ceta+%5CTheta_%5Cinfty%28x%2CX%29+%5Cnabla_%7Bf_t%28%5Ctheta%2CX%29%7D+%5Cmathcal%7BL%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;假设loss function是我们常见的Mean squared loss，那么方程进一步化简为一个线性常微分方程（ODE）：&lt;/p&gt;&lt;figure style="flex: 539.5348837209302" &gt;&lt;img width="464" height="43" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/bc6c7f594d98f5ef62651fcc3b5b1950.png" /&gt;&lt;figcaption&gt;image-20220622163732369&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当将函数一阶泰勒展开并保留一阶时，和上面ode等价，所以可以认为是一阶线性模型。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;名字由来tangent是因为取的是一阶泰勒导数&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK当宽度到($\frac{n}{d}=\Omega(d^{l-1})$）时可以近似到$+\infin$宽度&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;模型结构：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;何凯明initialization添加系数$\sqrt[2]{\frac{c_\sigma}{d_h}}$，其中$d_h$为h层神经元个数，这样保证每一层范数相等&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;研究方向：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;研究NTK的trainability and generalization， the NNGP (randomly initialized neural networks with certain class) kernel K and the tangent kernel Θ&lt;/li&gt;
&lt;/ol&gt;
</content><link href="/Blog/archives/neural%20tangent%20kernel%20%28NTK%29/" rel="alternate"/><published>2022-06-20T00:00:00+08:06</published></entry><entry><id>/Blog/archives/chinatown/</id><title>唐人街探案</title><updated>2022-07-18T02:38:00.724296+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;唐人街探案&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每个人内心都有善恶，每个人都有内心的不如意，但是每个人都要勇敢得去面对。&lt;/p&gt;&lt;p&gt;横移镜头有荒诞的喜剧效果，重复夸张的摔跤总是让人发笑，节奏把控的很好。&lt;/p&gt;</content><link href="/Blog/archives/chinatown/" rel="alternate"/><published>2022-06-20T00:00:00+08:06</published></entry><entry><id>/Blog/archives/the%20relax%20summer/</id><title>普罗旺斯的夏天</title><updated>2022-07-18T02:38:00.724258+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;普罗旺斯的夏天&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每个地方都有一个宁静而又祥和的一天，在普罗旺斯，能在早上看到天微微泛红，渐渐地展露新的一天，展现在人们面前最纯真的大自然。&lt;/p&gt;&lt;p&gt;第一天因为机考而睡不着觉，煎熬得度过这漫长的一天，渐渐地忘记睡觉，但是也是能够开始好好整理一下自己的生活，准备好前往下一个阶段的人生。&lt;/p&gt;</content><link href="/Blog/archives/the%20relax%20summer/" rel="alternate"/><published>2022-07-10T00:00:00+08:06</published></entry><entry><id>/Blog/archives/Paper%20Writing/</id><title>Paper Writing</title><updated>2022-07-18T02:38:00.724219+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;the craft of research&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;第三部分&lt;/p&gt;&lt;p&gt;how to make a claim？&lt;/p&gt;&lt;p&gt;how to let reader believe you？&lt;/p&gt;&lt;p&gt;尝试通过对话提前解决读者疑惑，说明工作的局限性&lt;/p&gt;&lt;p&gt;为了让故事更饱满，在边做的时候边写，让故事逻辑更丰满，让读者觉得你做的工作更有意义。&lt;/p&gt;</content><link href="/Blog/archives/Paper%20Writing/" rel="alternate"/><published>2022-07-15T00:00:00+08:06</published></entry><entry><id>/Blog/archives/lifefeeling/</id><title>人生感受</title><updated>2022-07-18T02:38:00.724178+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;人生感受&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果平常工作只是考虑表面，没有考虑自己是为了什么，如何实现自己的人生理想，最后也常常会因为不经意之间的鸡毛蒜皮感受到沮丧&lt;/p&gt;</content><link href="/Blog/archives/lifefeeling/" rel="alternate"/><published>2022-07-16T00:00:00+08:06</published></entry><entry><id>/Blog/archives/summer%20ghost/</id><title>夏日幻魂</title><updated>2022-07-18T02:38:00.724092+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;夏日幻魂&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;像友也一样，常常是因为学习占据了自己的绝大部分时间，来自社会，家人的期许往往让自己没了其他时间。他还能有自己一直坚持下去的热爱的东西，而像我自己却往往一点没有，总是提不起来很高昂的情绪，去投入自己所热爱的生活。&lt;/p&gt;&lt;p&gt;校园里接连不断的霸凌也让人无法安心地去上学，不经意间来自同学朋友间的羞辱，欺负，往往让自己无法勇敢去面对，自己无法拥有一个安心的环境去实现自己的梦。对青春期理性的冲动的艺术性放大，亦或是他们用过程完成了我们青春物语的残梦、用结果给我们指出了青春迷途的出路。&lt;/p&gt;&lt;p&gt;转瞬即逝的烟花，易逝的生命，理想、勇气，希望，这些熟悉的意像，较老套的就输剧情，鲜亮的画风，不错的运镜仍然给我们炎热浮躁的夏天增添了一份沉重、清凉，让我们能够静下心来读他人的故事，体味自己的人生。&lt;/p&gt;</content><link href="/Blog/archives/summer%20ghost/" rel="alternate"/><published>2022-07-18T00:00:00+08:06</published></entry></feed>