<?xml version='1.0' encoding='UTF-8'?>
<feed xmlns="http://www.w3.org/2005/Atom" xml:lang="zh-CN"><id>/Blog/</id><title>我的个人博客</title><updated>2021-07-28T09:20:31.648267+08:06</updated><author><name>LSQ</name><email>1959376918@qq.com</email></author><link href="/Blog/" rel="alternate"/><generator uri="https://lkiesow.github.io/python-feedgen" version="0.9.0">python-feedgen</generator><logo>https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/f-logo.png</logo><subtitle>记录生活美好</subtitle><entry><id>/Blog/archives/%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E6%80%BB%E7%BB%93/</id><title>pytorch代码可重复性</title><updated>2021-07-28T09:20:31.648666+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;训练的可重复性&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;CUDNN&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;cudnn中对卷积操作进行了优化，牺牲了精度来换取计算效率。如果需要保证可重复性，可以使用如下设置:&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;torch.backends&lt;/span&gt; &lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="n"&gt;cudnn&lt;/span&gt;
&lt;span class="n"&gt;cudnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;benchmark&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;False&lt;/span&gt;            &lt;span class="c1"&gt;# if benchmark=True, deterministic will be False&lt;/span&gt;
&lt;span class="n"&gt;cudnn&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;deterministic&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;True&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;不过实际上这个设置对精度影响不大，仅仅是小数点后几位的差别。所以如果不是对精度要求极高，其实不太建议修改，因为会使计算效率降低。&lt;/p&gt;&lt;ol start="2"&gt;
&lt;li&gt;&lt;strong&gt;Pytorch&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;            &lt;span class="c1"&gt;# 为CPU设置随机种子&lt;/span&gt;
  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;       &lt;span class="c1"&gt;# 为当前GPU设置随机种子&lt;/span&gt;
  &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;   &lt;span class="c1"&gt;# 为所有GPU设置随机种子&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;Python &amp;amp; Numpy&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;如果读取数据的过程采用了随机预处理(如RandomCrop、RandomHorizontalFlip等)，那么对python、numpy的随机数生成器也需要设置种子。&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;random&lt;/span&gt;
&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;
&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;最后，关于&lt;strong&gt;dataloader&lt;/strong&gt;：&lt;/p&gt;&lt;p&gt;注意，如果dataloader采用了多线程(num_workers &amp;gt; 1), 那么由于读取数据的顺序不同，最终运行结果也会有差异。也就是说，改变num_workers参数，也会对实验结果产生影响。目前暂时没有发现解决这个问题的方法，但是只要固定num_workers数目（线程数）不变，基本上也能够重复实验结果。&lt;/p&gt;&lt;p&gt;对于不同线程的随机数种子设置，主要通过DataLoader的worker_init_fn参数来实现。默认情况下使用线程ID作为随机数种子。如果需要自己设定，可以参考以下代码：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;GLOBAL_SEED&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;

&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;set_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;random&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
    &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;manual_seed_all&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;seed&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;GLOBAL_WORKER_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;
&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;worker_init_fn&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;worker_id&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;global&lt;/span&gt; &lt;span class="n"&gt;GLOBAL_WORKER_ID&lt;/span&gt;
    &lt;span class="n"&gt;GLOBAL_WORKER_ID&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;worker_id&lt;/span&gt;
    &lt;span class="n"&gt;set_seed&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;GLOBAL_SEED&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="n"&gt;worker_id&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;dataloader&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;DataLoader&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;dataset&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;batch_size&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;shuffle&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;True&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;num_workers&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;worker_init_fn&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;worker_init_fn&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</content><link href="/Blog/archives/%E5%8F%AF%E9%87%8D%E5%A4%8D%E6%80%A7%E6%80%BB%E7%BB%93/" rel="alternate"/><published>2021-07-18T00:00:00+08:06</published></entry><entry><id>/Blog/archives/model.train/</id><title>训练模式</title><updated>2021-07-28T09:20:31.648636+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;model.train()和model.eval()&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;model.train()&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;启用Batch Normalization 和 Dropout&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;model.train()是保证BN层能够用到每一批数据的均值和方差。对于Dropout，model.train()是随机取一部分网络连接来训练更新参数。照设定的参数p设置保留激活单元的概率（保留概率=p)&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;model.eval()&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;​	&lt;strong&gt;不启用 Batch Normalization 和 Dropout&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;model.eval()是保证BN层能够用全部训练数据的均值和方差，即测试过程中要保证BN层的均值和方差不变。对于Dropout，model.eval()是利用到了所有网络连接，即不进行随机舍弃神经元&lt;/p&gt;&lt;p&gt;dropout层会让&lt;em&gt;所有的激活单元都通过&lt;/em&gt;，而BN层会停止计算和更新mean和var，直接使用在训练阶段已经学出的&lt;em&gt;mean和var值&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;在model(test)之前，需要加上model.eval()，否则的话，有输入数据，即使不训练，它也会改变权值。这是model中含有BN层和Dropout所带来的的性质。该模式不会影响各层的gradient计算行为，即gradient计算和存储与training模式一样，只是不进行反向传播（back probagation)。&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;model.eval()和with torch.no_grad()的区别&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;with torch.no_grad()则主要是用于停止autograd模块的工作，以起到加速和节省显存的作用。它的作用是将该with语句包裹起来的部分停止梯度的更新，从而节省了GPU算力和显存，但是并不会影响dropout和BN层的行为&lt;/p&gt;&lt;p&gt;而model.eval()在不更新梯度的情况下会影响dropout和BN层的情况&lt;/p&gt;&lt;p&gt;以后调用model时提前声明模式&lt;/p&gt;</content><link href="/Blog/archives/model.train/" rel="alternate"/><published>2021-07-18T00:00:00+08:06</published></entry><entry><id>/Blog/archives/keng/</id><title>pytorch代码坑</title><updated>2021-07-28T09:20:31.648607+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;Pytorch的坑&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;nn.Module.cuda() 和 Tensor.cuda() 的作用效果差异&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;对于nn.Module:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt; 
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;对model自身进行的内存迁移。&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;对于tensor&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;和nn.Module不同，调用tensor.cuda()只是返回这个tensor对象在GPU内存上的拷贝，而不会对自身进行改变。因此必须对tensor进行重新赋值，即tensor=tensor.cuda().&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;create_a_model&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;zeros&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;10&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="c1"&gt;# 会报错&lt;/span&gt;
&lt;span class="n"&gt;tensor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cuda&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;tensor&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;    &lt;span class="c1"&gt;# 正常运行&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="2"&gt;
&lt;li&gt;&lt;strong&gt;PyTorch 0.4 计算累积损失的不同&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;以广泛使用的模式total_loss += loss.data[0]为例。Python0.4.0之前，loss是一个封装了(1,)张量的Variable，但Python0.4.0的loss现在是一个零维的标量。对标量进行索引是没有意义的（似乎会报 invalid index to scalar variable 的错误）。使用loss.item()可以从标量中获取Python数字。所以改为：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;total_loss&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;torch.Tensor.detach()的使用&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;detach()的官方说明如下：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;Returns&lt;/span&gt; &lt;span class="n"&gt;a&lt;/span&gt; &lt;span class="n"&gt;new&lt;/span&gt; &lt;span class="n"&gt;Tensor&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;detached&lt;/span&gt; &lt;span class="kn"&gt;from&lt;/span&gt; &lt;span class="nn"&gt;the&lt;/span&gt; &lt;span class="n"&gt;current&lt;/span&gt; &lt;span class="n"&gt;graph&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
    &lt;span class="n"&gt;The&lt;/span&gt; &lt;span class="n"&gt;result&lt;/span&gt; &lt;span class="n"&gt;will&lt;/span&gt; &lt;span class="n"&gt;never&lt;/span&gt; &lt;span class="n"&gt;require&lt;/span&gt; &lt;span class="n"&gt;gradient&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;假设有模型A和模型B，我们需要将A的输出作为B的输入，但训练时我们只训练模型B. 那么可以这样做：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;input_B&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;output_A&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;detach&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;它可以使两个计算图的梯度传递断开，从而实现我们所需的功能。&lt;/p&gt;&lt;ol start="4"&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ERROR: Unexpected bus error encountered in worker. This might be caused by insufficient shared memory (shm)&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;出现这个错误的情况是，在服务器上的docker中运行训练代码时，batch size设置得过大，shared memory不够（因为docker限制了shm）.解决方法是，将Dataloader的num_workers设置为0.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;pytorch中loss函数的参数设置&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;以CrossEntropyLoss为例：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;CrossEntropyLoss&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;size_average&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ignore_index&lt;/span&gt;&lt;span class="o"&gt;=-&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduce&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;reduction&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;elementwise_mean&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;若 reduce = False，那么 size_average 参数失效，直接返回向量形式的 loss，即batch中每个元素对应的loss.&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;若 reduce = True，那么 loss 返回的是标量：&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;ul&gt;
&lt;li&gt;如果 size_average = True，返回 loss.mean().&lt;/li&gt;
&lt;li&gt;如果 size_average = False，返回 loss.sum().&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;weight : 输入一个1D的权值向量，为各个类别的loss加权，如下公式所示：&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;figure style="flex: 433.5820895522388" &gt;&lt;img width="581" height="67" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/cb9a8b8fefac1651f91f96e109f6a66c.jpg" /&gt;&lt;/figure&gt;&lt;ul&gt;
&lt;li&gt;ignore_index : 选择要忽视的目标值，使其对输入梯度不作贡献。如果 size_average = True，那么只计算不被忽视的目标的loss的均值。&lt;/li&gt;
&lt;li&gt;reduction : 可选的参数有：‘none’ | ‘elementwise_mean’ | ‘sum’, 正如参数的字面意思，不解释。&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;多GPU的处理机制&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;使用多GPU时，应该记住pytorch的处理逻辑是：&lt;/p&gt;&lt;p&gt;1)在各个GPU上初始化模型。&lt;/p&gt;&lt;p&gt;2)前向传播时，把batch分配到各个GPU上进行计算。&lt;/p&gt;&lt;p&gt;3)得到的输出在主GPU上进行汇总，计算loss并反向传播，更新主GPU上的权值。&lt;/p&gt;&lt;p&gt;4)把主GPU上的模型复制到其它GPU上&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;num_batches_tracked参数&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;读取模型参数时出现了错误&lt;/p&gt;&lt;p&gt;KeyError: 'unexpected key &amp;quot;module.bn1.num_batches_tracked&amp;quot; in state_dict'&lt;/p&gt;&lt;p&gt;经过研究发现，在pytorch 0.4.1及后面的版本里，BatchNorm层新增了num_batches_tracked参数，用来统计训练时的forward过的batch数目，源码如下（pytorch0.4.1）：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;training&lt;/span&gt; &lt;span class="ow"&gt;and&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;track_running_stats&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
        &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_batches_tracked&lt;/span&gt; &lt;span class="o"&gt;+=&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
        &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# use cumulative moving average&lt;/span&gt;
            &lt;span class="n"&gt;exponential_average_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mf"&gt;1.0&lt;/span&gt; &lt;span class="o"&gt;/&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;num_batches_tracked&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
        &lt;span class="k"&gt;else&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;  &lt;span class="c1"&gt;# use exponential moving average&lt;/span&gt;
            &lt;span class="n"&gt;exponential_average_factor&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;momentum&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;大概可以看出，这个参数和训练时的归一化的计算方式有关。&lt;/p&gt;&lt;p&gt;因此，我们可以知道该错误是由于训练和测试所用的pytorch版本(0.4.1版本前后的差异)不一致引起的。具体的解决方案是：如果是模型参数（Orderdict格式，很容易修改）里少了num_batches_tracked变量，就加上去，如果是多了就删掉。偷懒的做法是将load_state_dict的strict参数置为False，如下所示：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;load_state_dict&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;load&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;weight_path&lt;/span&gt;&lt;span class="p"&gt;),&lt;/span&gt; &lt;span class="n"&gt;strict&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;False&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;还看到有人直接修改pytorch 0.4.1的源代码把num_batches_tracked参数删掉的，这就非常不建议了。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;训练时损失出现nan的问题&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;训练模型时出现了损失为nan的情况，发现是个大坑。&lt;/p&gt;&lt;p&gt;可能导致梯度出现nan的三个原因：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1.梯度爆炸&lt;/strong&gt;。也就是说梯度数值超出范围变成nan. 通常可以调小学习率、加BN层或者做梯度裁剪来试试看有没有解决。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.损失函数或者网络设计。&lt;/strong&gt;比方说，出现了除0，或者出现一些边界情况导致函数不可导，比方说log(0)、sqrt(0).&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.脏数据。&lt;/strong&gt;可以事先对输入数据进行判断看看是否存在nan.&lt;/p&gt;&lt;p&gt;补充一下nan数据的判断方法：&lt;/p&gt;&lt;p&gt;注意！像nan或者inf这样的数值不能使用 == 或者 is 来判断！为了安全起见统一使用 math.isnan() 或者 numpy.isnan() 吧。&lt;/p&gt;&lt;p&gt;例如：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="kn"&gt;import&lt;/span&gt; &lt;span class="nn"&gt;numpy&lt;/span&gt; &lt;span class="k"&gt;as&lt;/span&gt; &lt;span class="nn"&gt;np&lt;/span&gt;

&lt;span class="c1"&gt;# 判断输入数据是否存在nan&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;any&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;input&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;cpu&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;numpy&lt;/span&gt;&lt;span class="p"&gt;())):&lt;/span&gt;
  &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Input data has NaN!&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="c1"&gt;# 判断损失是否为nan&lt;/span&gt;
&lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;np&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;isnan&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;loss&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;item&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
  &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;Loss value is NaN!&amp;#39;&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;ValueError: Expected more than 1 value per channel when training&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;当batch里只有一个样本时，再调用batch_norm就会报下面这个错误：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;raise ValueError(&amp;#39;Expected more than 1 value per channel when training, got input size {}&amp;#39;.format(size))
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;没有什么特别好的解决办法，在训练前用 num_of_samples % batch_size 算一下会不会正好剩下一个样本。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;优化器的weight_decay项导致的隐蔽bug&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;weight_decay指的是权值衰减，即在原损失的基础上加上一个L2惩罚项，使得模型趋向于选择更小的权重参数，起到正则化的效果。&lt;/p&gt;&lt;p&gt;在训练一个ResNet50的时候，网络的高层部分layer4暂时没有用到，因此也并不会有梯度回传，于是将ResNet50的&lt;strong&gt;所有参数&lt;/strong&gt;都传递给Optimizer进行更新了，想着layer4应该能保持原来的权重不变才对。但是实际上，尽管layer4没有梯度回传，但是&lt;strong&gt;weight_decay的作用仍然存在，它使得layer4权值越来越小，趋向于0&lt;/strong&gt;。后面需要用到layer4的时候，发现输出异常（接近于0），才注意到这个问题的存在。&lt;/p&gt;&lt;p&gt;虽然这样的情况可能不容易遇到，但是还是要谨慎：&lt;strong&gt;暂时不需要更新的权值，一定不要传递给Optimizer，避免不必要的麻烦。&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
</content><link href="/Blog/archives/keng/" rel="alternate"/><published>2021-07-18T00:00:00+08:06</published></entry><entry><id>/Blog/archives/lr%E2%80%94%E2%80%94optimization/</id><title>pytorch学习率调整</title><updated>2021-07-28T09:20:31.648570+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;一、pytorch中学习率调整方法&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;1. lr_scheduler.StepLR&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;class torch.optim.lr_scheduler.StepLR(optimizer, step_size, gamma=0.1, last_epoch=-1)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt; 等间隔调整学习率，&lt;strong&gt;调整倍数为gamma倍，调整间隔为step_size&lt;/strong&gt;。间隔单位是step。需要注意的是，step通常是指epoch，不要弄成iteration了。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;step_size(int)- 学习率下降间隔数，若为30，则会在30、60、90......个step时，将*&lt;em&gt;学习率调整为lr&lt;/em&gt;gamma**。&lt;/p&gt;&lt;p&gt;gamma(float)- 学习率调整倍数，&lt;strong&gt;默认为0.1倍，即下降10倍&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;last_epoch(int)- &lt;strong&gt;上一个epoch数&lt;/strong&gt;，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整。&lt;strong&gt;当为-1时，学习率设置为初始值&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.lr_scheduler.MultiStepLR&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;class torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones, gamma=0.1, last_epoch=-1)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt; 按设定的间隔调整学习率。这个方法适合后期调试使用，==观察loss曲线==，为每个实验定制学习率调整时机。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;milestones(list)- 一个list，==每一个元素代表何时调整学习率==，list元素必须是递增的。如 milestones=[30,80,120]&lt;/p&gt;&lt;p&gt;gamma(float)- 学习率调整倍数，默认为0.1倍，即下降10倍。&lt;/p&gt;&lt;p&gt;last_epoch(int)- 上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整。当为-1时，学习率设置为初始值。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;3.lr_scheduler.ExponentialLR&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;class torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma, last_epoch=-1)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt; 按指数衰减调整学习率，调整公式: lr = lr * gamma** &lt;strong&gt;epoch&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;参数：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;gamma- 学习率调整倍数的底，指数为epoch，即 gamma*&lt;/strong&gt;*epoch&lt;/p&gt;&lt;p&gt;last_epoch(int)- 上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整。当为-1时，学习率设置为初始值。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;4.lr_scheduler.CosineAnnealingLR&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;class torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max, eta_min=0, last_epoch=-1)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt; 以余弦函数为周期，并在每个周期最大值时重新设置学习率。具体如下图所示&lt;/p&gt;&lt;figure style="flex: 77.92207792207792" &gt;&lt;img width="720" height="462" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/766ae875366d7fedfd1e5d0ccba04d34.jpg" /&gt;&lt;/figure&gt;&lt;p&gt;详细请阅读论文《 SGDR: Stochastic Gradient Descent with Warm Restarts》(ICLR-2017)：&lt;a href="https://link.zhihu.com/?target=https%3A//arxiv.org/abs/1608.03983"&gt;&lt;a href="https://arxiv.org/abs/1608.03983"&gt;https://arxiv.org/abs/1608.03983&lt;/a&gt;&lt;/a&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;T_max(int)- 一次学习率周期的迭代次数，即T_max个epoch之后重新设置学习率。 eta_min(float)- 最小学习率，即在一个周期中，学习率最小会下降到eta_min，默认值为0。&lt;/p&gt;&lt;p&gt;学习率调整公式为：&lt;/p&gt;&lt;figure style="flex: 278.46153846153845" &gt;&lt;img width="362" height="65" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/708c59d860cae21f9926274afe951a8d.jpg" /&gt;&lt;/figure&gt;&lt;p&gt;可以看出是以初始学习率为最大学习率，以2*Tmax为周期，在一个周期内先下降，后上升。&lt;/p&gt;&lt;p&gt;实例： T_max = 200, 初始学习率 = 0.001, eta_min = 0&lt;/p&gt;&lt;figure style="flex: 76.57342657342657" &gt;&lt;img width="219" height="143" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/df4aea5436368aa7d5e63c326bab902c.jpg" /&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;5.lr_scheduler.ReduceLROnPlateau&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;class torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10, verbose=False, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0, eps=1e-08)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt; 当某指标不再变化（下降或升高），调整学习率，这是非常实用的学习率调整策略。例如，当验证集的loss不再下降时，进行学习率调整；或者监测验证集的accuracy，当accuracy不再上升时，则调整学习率。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;mode(str)- 模式选择，有 min和max两种模式，min表示当指标不再降低(如监测loss)，max表示当指标不再升高(如监测accuracy)。&lt;/p&gt;&lt;p&gt;factor(float)- 学习率调整倍数(等同于其它方法的gamma)，即学习率更新为 lr = lr * factor patience(int)- 直译——&amp;quot;耐心&amp;quot;，即忍受该指标多少个step不变化，当忍无可忍时，调整学习率。注，可以不是连续5次。&lt;/p&gt;&lt;p&gt;verbose(bool)- 是否打印学习率信息， print('Epoch {:5d}: reducing learning rate' ' of group {} to {:.4e}.'.format(epoch, i, new_lr))&lt;/p&gt;&lt;p&gt;threshold(float)- Threshold for measuring the new optimum，配合threshold_mode使用，默认值1e-4。作用是用来控制当前指标与best指标的差异。&lt;/p&gt;&lt;p&gt;threshold_mode(str)- 选择判断指标是否达最优的模式，有两种模式，rel和abs。 当threshold_mode = rel，并且mode = max时，dynamic_threshold = best * ( 1 + threshold )； 当threshold_mode = rel，并且mode = min时，dynamic_threshold = best * ( 1 - threshold )； 当threshold_mode = abs，并且mode = max时，dynamic_threshold = best + threshold ； 当threshold_mode = rel，并且mode = max时，dynamic_threshold = best - threshold&lt;/p&gt;&lt;p&gt;cooldown(int)- “冷却时间“，当调整学习率之后，让学习率调整策略冷静一下，让模型再训练一段时间，再重启监测模式。&lt;/p&gt;&lt;p&gt;min_lr(float or list)- 学习率下限，可为float，或者list，当有多个参数组时，可用list进行设置。&lt;/p&gt;&lt;p&gt;eps(float)- 学习率衰减的最小值，当学习率变化小于eps时，则不调整学习率。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;6.lr_scheduler.LambdaLR&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;class torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda, last_epoch=-1)&lt;/p&gt;&lt;p&gt;&lt;strong&gt;功能：&lt;/strong&gt; 为不同参数组设定不同学习率调整策略。调整规则为，lr = base_lr * lmbda(self.last_epoch) 。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;参数：&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;lr_lambda(function or list)- 一个计算&lt;strong&gt;学习率调整倍数的函数&lt;/strong&gt;，输入通常为step，当有多个参数组时，设为list。&lt;/p&gt;&lt;p&gt;last_epoch(int)- 上一个epoch数，这个变量用来指示学习率是否需要调整。当last_epoch符合设定的间隔时，就会对学习率进行调整。当为-1时，学习率设置为初始值。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;例如：&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;ignored_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;list&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;map&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;()))&lt;/span&gt;
&lt;span class="n"&gt;base_params&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="nb"&gt;filter&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;p&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="nb"&gt;id&lt;/span&gt;&lt;span class="err"&gt;§&lt;/span&gt; &lt;span class="ow"&gt;not&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;ignored_params&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;SGD&lt;/span&gt;&lt;span class="p"&gt;([&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;base_params&lt;/span&gt;&lt;span class="p"&gt;},&lt;/span&gt;
    &lt;span class="p"&gt;{&lt;/span&gt;&lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="n"&gt;params&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;net&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;fc3&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;}],&lt;/span&gt; 
    &lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;momentum&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;0.9&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;weight_decay&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="mf"&gt;1e-4&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;

&lt;span class="n"&gt;lambda1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt;
&lt;span class="n"&gt;lambda2&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="k"&gt;lambda&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mf"&gt;0.95&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;

&lt;span class="n"&gt;scheduler&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;torch&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;lr_scheduler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;LambdaLR&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr_lambda&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;lambda1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lambda2&lt;/span&gt;&lt;span class="p"&gt;])&lt;/span&gt;
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;range&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;100&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="n"&gt;scheduler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
&lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s1"&gt;&amp;#39;epoch: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;i&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="s1"&gt;&amp;#39;lr: &amp;#39;&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;scheduler&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_lr&lt;/span&gt;&lt;span class="p"&gt;())&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;strong&gt;输出：&lt;/strong&gt;&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.095&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;2&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.09025&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;3&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0857375&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;4&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.081450625&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;5&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.001&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.07737809374999999&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;6&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.002&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.07350918906249998&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;7&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.002&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.06983372960937498&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;8&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.002&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.06634204312890622&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="mi"&gt;9&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mf"&gt;0.003&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="mf"&gt;0.0630249409724609&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;第一个参数组的学习率为0：这是因为学习率计算方式。 第一个参数组的初始学习率设置为0.001, lambda1 = lambda epoch: epoch // 3, 第1个epoch时，由lr = base_lr * lmbda(self.last_epoch)，可知道 lr = 0.001 * (0//3) ，又因为1//3等于0，所以导致学习率为0。&lt;/p&gt;&lt;p&gt;第二个参数组的学习率变化，初始为0.1，lr = 0.1 * 0.95^epoch ，当epoch为0时，lr=0.1 ，epoch为1时，lr=0.1*0.95。&lt;/p&gt;&lt;hr /&gt;
&lt;p&gt;&lt;strong&gt;二、 学习率调整小结及step源码阅读&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.1 学习率调整小结&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Pytorch提供了六种学习率调整方法，可分为三大类，分别是&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;有序调整；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;自适应调整；&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;自定义调整。&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;第一类，依一定规律有序进行调整，这一类是最常用的，分别是等间隔下降(Step)，按需设定下降间隔(MultiStep)，指数下降(Exponential)和CosineAnnealing。这四种方法的调整时机都是人为可控的，也是&lt;strong&gt;训练时常用到的&lt;/strong&gt;。&lt;/p&gt;&lt;p&gt;第二类，依训练状况伺机调整，这就是ReduceLROnPlateau方法。该法通过监测某一指标的变化情况，当该指标不再怎么变化的时候，就是调整学习率的时机，因而属于自适应的调整。&lt;/p&gt;&lt;p&gt;第三类，自定义调整，Lambda。Lambda方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在fine-tune中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略，简直不能更棒！&lt;/p&gt;&lt;p&gt;&lt;strong&gt;2.2 step源码阅读&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;在pytorch中，学习率的更新是通过scheduler.step()，而我们知道影响学习率的一个重要参数就是epoch，而epoch与scheduler.step()是如何关联的呢？这就需要看源码了。 源码在torch/optim/lr_scheduler.py，step()方法在_LRScheduler类当中，该类作为所有学习率调整的基类，其中定义了一些基本方法，如现在要介绍的step()，以及最常用的get_lr()，不过get_lr()是一个虚函数，均需要在派生类中重新定义函数。&lt;/p&gt;&lt;p&gt;看看step()&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;step&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="ow"&gt;is&lt;/span&gt; &lt;span class="kc"&gt;None&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="n"&gt;epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last_epoch&lt;/span&gt; &lt;span class="o"&gt;+&lt;/span&gt; &lt;span class="mi"&gt;1&lt;/span&gt;
    &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last_epoch&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;epoch&lt;/span&gt;
    &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;param_group&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="nb"&gt;zip&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;optimizer&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;param_groups&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;get_lr&lt;/span&gt;&lt;span class="p"&gt;()):&lt;/span&gt;
    	&lt;span class="n"&gt;param_group&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="err"&gt;‘&lt;/span&gt;&lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="err"&gt;’&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;函数接收变量epoch，默认为None，当为None时，epoch = self.last_epoch + 1。从这里知道，last_epoch是用以记录epoch的。上面有提到last_epoch的初始值是-1，因此，第一个epoch的值为 -1+1 =0。接着最重要的一步就是获取学习率，并更新。 由于pytorch是基于参数组的管理方式，这里需要采用for循环对每一个参数组的学习率进行获取及更新。这里需要注意的是get_lr()，get_lr()的功能就是获取当前epoch，该参数组的学习率。&lt;/p&gt;&lt;p&gt;这里以StepLR()为例，介绍get_lr()，请看代码：&lt;/p&gt;&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;def&lt;/span&gt; &lt;span class="nf"&gt;get_lr&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="p"&gt;):&lt;/span&gt;
    &lt;span class="k"&gt;return&lt;/span&gt; &lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;base_lr&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gamma&lt;/span&gt; &lt;span class="o"&gt;**&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;last_epoch&lt;/span&gt; &lt;span class="o"&gt;//&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;step_size&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;base_lr&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="bp"&gt;self&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;base_lrs&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;由于pytorch是基于参数组的管理方式，可能会有多个参数组，因此用for循环，返回的是一个list。list元素的计算方式为 base_lr * self.gamma ** (self.last_epoch // self.step_size)。&lt;/p&gt;&lt;p&gt;看完代码，可以知道，在执行一次scheduler.step()之后，epoch会加1，因此scheduler.step()要放在epoch的for循环当中执行。&lt;/p&gt;&lt;hr /&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="c1"&gt;#优化函数，model.parameters()为该实例中可优化的参数，lr为参数优化的选项（学习率等）&lt;/span&gt;
&lt;span class="n"&gt;optimizer&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;optim&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;Adam&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;parameters&lt;/span&gt;&lt;span class="p"&gt;(),&lt;/span&gt; &lt;span class="n"&gt;lr&lt;/span&gt;&lt;span class="o"&gt;=&lt;/span&gt;&lt;span class="n"&gt;learning_rate&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; 
&lt;span class="k"&gt;for&lt;/span&gt; &lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt; &lt;span class="ow"&gt;in&lt;/span&gt; &lt;span class="n"&gt;model&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;named_parameters&lt;/span&gt;&lt;span class="p"&gt;():&lt;/span&gt; &lt;span class="c1"&gt;#查看可优化的参数有哪些&lt;/span&gt;
  &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="n"&gt;param&lt;/span&gt;&lt;span class="o"&gt;.&lt;/span&gt;&lt;span class="n"&gt;requires_grad&lt;/span&gt;&lt;span class="p"&gt;:&lt;/span&gt;
    &lt;span class="nb"&gt;print&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;name&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;
</content><link href="/Blog/archives/lr%E2%80%94%E2%80%94optimization/" rel="alternate"/><published>2021-07-19T00:00:00+08:06</published></entry><entry><id>/Blog/archives/strange%20idea/</id><title>20号奇怪想法</title><updated>2021-07-28T09:20:31.648535+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;七月二十日&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;KD蒸馏的新奇地方&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;response-base knowledge：用最后的logits，最简单直接&lt;/li&gt;
&lt;li&gt;feature-base knowledge：loss方法，hint layer的选取没有明确的指示，让中间层的特征，attention map一致&lt;/li&gt;
&lt;li&gt;relation-base knowledge：同时使用logits和中间层的feature&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;训练方法，offline，online，self-distillation&lt;/p&gt;&lt;p&gt;stu-network设计暂时不设计&lt;/p&gt;&lt;p&gt;训练算法：使用gan，生成数据，增强数据集&lt;/p&gt;&lt;p&gt;多种模型压缩算法使用增强网络功能&lt;/p&gt;&lt;p&gt;小的教师网络增强大的学生网络的学习&lt;/p&gt;&lt;p&gt;feature-base knowledge distillation开源文章及代码：&lt;/p&gt;&lt;p&gt;&lt;strong&gt;Cross-Layer Distillation with Semantic Calibration&lt;/strong&gt;（&lt;a href="https://github.com/DefangChen/SemCKD"&gt;GitHub - DefangChen/SemCKD: This is the official implementation for the AAAI-2021 paper (Cross-Layer Distillation with Semantic Calibration).&lt;/a&gt;  AAAI被引用次数1&lt;/p&gt;&lt;p&gt;2014-2021KD论文整理：&lt;a href="https://github.com/FLHonker/Awesome-Knowledge-Distillation"&gt;FLHonker/Awesome-Knowledge-Distillation: Awesome Knowledge-Distillation. 分类整理的知识蒸馏paper(2014-2021)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;attetionmap use neuron selectivity transfer&lt;/p&gt;</content><link href="/Blog/archives/strange%20idea/" rel="alternate"/><published>2021-07-20T00:00:00+08:06</published></entry><entry><id>/Blog/archives/key_point/</id><title>论文阅读</title><updated>2021-07-28T09:20:31.648505+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;论文阅读&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;总共四篇论文，关于action recognition蒸馏方向&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Spatiotemporal distilled dense-Connectivity network for video action recognition&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;解决空间和时间层面信息共享的问题，而不是单独训练。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;新知识&lt;/strong&gt;：乘法门&lt;/p&gt;&lt;figure style="flex: 93.26424870466322" &gt;&lt;img width="720" height="386" src="https://pic3.zhimg.com/80/v2-a4142c9523b06fd73b9190d6f36625e2_720w.jpg" /&gt;&lt;/figure&gt;&lt;p&gt;乘法门是一组信息对另一组数据的控制。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;写作手法&lt;/strong&gt;：首先便与21文章比对区别之处，说明网络想法来源，通过DenseNet（将所有浅层输出的feature map作为输入），方便信息重复利用，减缓梯度消失问题，等等。公式花里胡哨的，将公式分多块书写，每个loss大量笔墨&lt;/p&gt;&lt;p&gt;新设计的网络跨模态融合，让RGB（appearance）和FLOW（motion）信息交流，设计了一个新的网络结构SDDN和新的蒸馏方式，两个学生（RGB，FLOW）和一个teacher（fusion）&lt;/p&gt;&lt;figure style="flex: 103.07308970099668" &gt;&lt;img width="1241" height="602" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/e6fc8b44286ff20c27bf6bf31bb6ac6b.png" /&gt;&lt;figcaption&gt;image-20210722105903240&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;基础网络结构&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如果使用乘法将两个信息相乘进行控制？使用dense connection？&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;用所有层的信息有提升准确率&lt;/p&gt;&lt;figure style="flex: 232.5" &gt;&lt;img width="372" height="80" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/3cc938b889631727b1c59843a1d2b9ca.png" /&gt;&lt;figcaption&gt;image-20210722115733057&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;将各个层的flow feature concatenate在一起&lt;/p&gt;&lt;p&gt;消融实验做得很好：&lt;/p&gt;&lt;p&gt;提升的做法有：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;STDDCN模型本身更好&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;α，β，temperature选取最佳值&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Distillation，教师和两个学生的模式更佳&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;利用前置所有的信息更佳&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;motion信息指导appearance更佳&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;Modality distillation with multiple stream networks for action recognition&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;hallucination network:初步理解为模拟的信息输入，自我产生，例如在RGB中模拟输入depth信息 &lt;strong&gt;是否可以模拟帧数多的情景&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;将depth数据学得的信息输入hallucination network中学习，网络原本的输入是RGB，所以最后可以在同一种RGB数据下训练和测试&lt;/p&gt;&lt;p&gt;跨模态方法是通过残差&lt;/p&gt;&lt;figure style="flex: 347.43589743589746" &gt;&lt;img width="542" height="78" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/976d481f370ea18695f14d48ca09d38a.png" /&gt;&lt;figcaption&gt;image-20210722195112871&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;Cross-Modal Knowledge Distillation for Action Recognition&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;使用交叉熵，获得比KL-loss更高的准确度，原因是temperature不好确定&lt;/p&gt;&lt;p&gt;mutual learning 使用多个学生网络&lt;/p&gt;&lt;figure style="flex: 91.98250728862973" &gt;&lt;img width="631" height="343" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/d467e9d6ba401d7fcb9e8fc3e7a0bc2c.png" /&gt;&lt;figcaption&gt;image-20210723101729283&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;解决的问题，将没有标注的skeleton data用于学生网络的训练，增强了数据集的可用性，（可不可以直接用teacher 网络的输出作为ground truth，这样的准确率等于0.86*0.86=0.7396）mutual learning 74.2差不多&lt;/p&gt;&lt;p&gt;利用了缺失标签的信息才是它的长处&lt;/p&gt;&lt;ol start="4"&gt;
&lt;li&gt;&lt;strong&gt;Graph distillation for action detection with privileged modalities&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;解决问题：source domain大部分信息没有利用到&lt;/p&gt;&lt;p&gt;human parsing:人体部分的颜色标注&lt;/p&gt;&lt;p&gt;前面层低级语义信息可以共享&lt;/p&gt;&lt;p&gt;1*1卷积：局部卷积，信息充分融合&lt;/p&gt;&lt;p&gt;训练方式，数据集&lt;/p&gt;&lt;p&gt;会议更新的比较快&lt;/p&gt;&lt;p&gt;bilstm：帧顺着输一次，倒着输一次，可以得到后面的信息&lt;/p&gt;&lt;p&gt;LSTM学习到后面的隐变量&lt;/p&gt;&lt;p&gt;HDM51 dataloader&lt;/p&gt;</content><link href="/Blog/archives/key_point/" rel="alternate"/><published>2021-07-22T00:00:00+08:06</published></entry><entry><id>/Blog/archives/resnet/</id><title>resnet网络</title><updated>2021-07-28T09:20:31.648473+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;restnet&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;简化了学习过程，增强了梯度传播&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;相比于学习原始的信号，残差网络学习的是信号的差值，这在许多的研究中被验证是更加有效的，它简化了学习的过程。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;根据我们前面的内容可知，在一定程度上，网络越深表达能力越强，性能越好。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;然而随着网络深度的增加，带来了许多优化相关的问题，比如梯度消散，梯度爆炸。&lt;/p&gt;&lt;p&gt;残差网络从根本上解决了梯度问题&lt;/p&gt;&lt;figure style="flex: 121.0762331838565" &gt;&lt;img width="1080" height="446" src="https://res-static.hc-cdn.cn/fms/img/fb9d7c7db3bb265f7f997160aa48bc641603780769781" /&gt;&lt;/figure&gt;&lt;ol start="2"&gt;
&lt;li&gt;&lt;strong&gt;打破了网络的不对称性&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;虽然残差网络可以通过跳层连接，增强了梯度的流动，从而使得上千层网络的训练成为可能，&lt;strong&gt;不过相关的研究表面残差网络的有效性，更加体现在减轻了神经网络的退化。&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;如果在网络中每个层只有少量的隐藏单元对不同的输入改变它们的激活值，而大部分隐藏单元对不同的输入都是相同的反应，此时整个权重矩阵的秩不高。并且随着网络层数的增加，连乘后使得整个秩变的更低，这就是我们常说的网络退化问题。&lt;/strong&gt;&lt;/p&gt;&lt;ol start="3"&gt;
&lt;li&gt;&lt;strong&gt;增强了网络的泛化能力&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;一个残差块可以用表示为：&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D%3D+x_l%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B1%7D" /&gt;&lt;/figure&gt;&lt;p&gt;残差块分成两部分直接映射部分和残差部分。 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=h%28x_l%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是直接映射，反应在图1中是左边的曲线； &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是残差部分，一般由两个或者三个卷积操作构成，即下图中右侧包含卷积的部分。&lt;/p&gt;&lt;figure style="flex: 23.668639053254438" &gt;&lt;img width="160" height="338" src="https://pic2.zhimg.com/80/v2-bd76d0f10f84d74f90505eababd3d4a1_720w.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;上图中的Weight在卷积网络中是指卷积操作，addition是指单位加操作。&lt;/p&gt;&lt;p&gt;&lt;strong&gt;残差网络的背后原理&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;残差块一个更通用的表示方式是&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=y_l%3D+h%28x_l%29%2B%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B3%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+f%28y_l%29%5Ctag%7B4%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;现在我们先不考虑升维或者降维的情况，那么在[1]中， &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=h%28%5Ccdot%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是直接映射， &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=f%28%5Ccdot%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是激活函数，一般使用ReLU。我们首先给出两个假设：&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;假设1： &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=h%28%5Ccdot%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是直接映射；&lt;/li&gt;
&lt;li&gt;假设2： &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=f%28%5Ccdot%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是直接映射。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;那么这时候残差块可以表示为：&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+x_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B5%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;对于一个更深的层 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=L" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ，其与 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=l" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 层的关系可以表示为&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_L+%3D+x_l+%2B+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%5Ctag%7B6%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;这个公式反应了残差网络的两个属性：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=L" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 层可以表示为==任意一个比它浅的l层和他们之间的残差部分之和；==&lt;/li&gt;
&lt;li&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_L%3D+x_0+%2B+%5Csum_%7Bi%3D0%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ， &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=L" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 是各个残差块特征的单位累和，而MLP是特征矩阵的累积。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;根据BP（back propagation）中使用的导数的链式法则，损失函数 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cvarepsilon" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 关于 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_l" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 的梯度可以表示为&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%5Cfrac%7B%5Cpartial+x_L%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%281%2B%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%29+%3D+%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D%2B%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D+%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29+%5Ctag%7B7%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;上面公式反映了残差网络的两个属性：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;在整个训练过程中， &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%7D%7B%5Cpartial+x_l%7D%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29+" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 不可能一直为 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=-1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ，也就是说在残差网络中==不会出现梯度消失的问题==。&lt;/li&gt;
&lt;li&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 表示 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=L" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 层的梯度可以直接传递到任何一个比它浅的 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=l" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 层。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;通过分析残差网络的正向和反向两个过程，我们发现，当残差块满足上面两个假设时，信息可以非常畅通的在高层和低层之间相互传导，说明这两个假设是让残差网络可以训练深度模型的充分条件。那么这两个假设是必要条件吗？&lt;/p&gt;&lt;p&gt;&lt;strong&gt;直接映射是最好的选择&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;对于假设1，我们采用反证法，假设 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=h%28x_l%29+%3D+%5Clambda_l+x_l" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ，那么这时候，残差块（图3.b）表示为&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_%7Bl%2B1%7D+%3D+%5Clambda_lx_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+%7BW_l%7D%29%5Ctag%7B8%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;对于更深的L层&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_%7BL%7D+%3D+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_i%29x_l+%2B+%5Csum_%7Bi%3Dl%7D%5E%7BL-1%7D%5Cmathcal%7BF%7D%28x_i%2C+%7BW_i%7D%29%5Ctag%7B9%7D" /&gt;&lt;/figure&gt;&lt;p&gt;为了简化问题，我们只考虑公式的左半部分 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_%7BL%7D+%3D+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_l%29x_l" /&gt;&lt;/figure&gt; ，损失函数 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cvarepsilon" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 对 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=x_l" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 求偏微分得&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial%5Cvarepsilon%7D%7B%5Cpartial+x_l%7D+%3D+%5Cfrac%7B%5Cpartial%5Cvarepsilon%7D%7B%5Cpartial+x_L%7D+%5Cleft%28+%28%5Cprod_%7Bi%3Dl%7D%5E%7BL-1%7D%5Clambda_i%29+%2B+%5Cfrac%7B%5Cpartial%7D%7B%5Cpartial+x_l%7D+%5Chat%7B%5Cmathcal%7BF%7D%7D%28x_i%2C+%5Cmathcal%7BW%7D_i%29%5Cright%29%5Ctag%7B10%7D+" /&gt;&lt;/figure&gt;&lt;p&gt;上面公式反映了两个属性：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;当 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Clambda%3E1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 时，很有可能发生梯度爆炸；&lt;/li&gt;
&lt;li&gt;当 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Clambda%3C1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 时，梯度变成0，会阻碍残差网络信息的反向传递，从而影响残差网络的训练。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Clambda" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ==必须等1==。同理，其他常见的激活函数都会产生和上面的例子类似的阻碍信息反向传播的问题。&lt;/p&gt;&lt;p&gt;对于其它不影响梯度的 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=h%28%5Ccdot%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ，例如LSTM中的门机制（图3.c，图3.d）或者Dropout（图3.f）以及[1]中用于降维的 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=1%5Ctimes1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 卷积（图3.e）也许会有效果，作者采用了实验的方法进行验证，实验结果见图4&lt;/p&gt;&lt;figure style="flex: 52.63157894736842" &gt;&lt;img width="720" height="684" src="https://pic2.zhimg.com/80/v2-843326b572e2e4c5c8956e289bd3f58d_720w.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;图3：直接映射的变异模型&lt;/p&gt;&lt;figure style="flex: 109.7560975609756" &gt;&lt;img width="720" height="328" src="https://pic4.zhimg.com/80/v2-5d8fd2868a4ba30e61ce477ab00d7f0f_720w.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;图4：变异模型（均为110层）在Cifar10数据集上的表现&lt;p&gt;从图4的实验结果中我们可以看出，在所有的变异模型中，依旧是==直接映射的效果最好==。下面我们对图3中的各种变异模型的分析&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;Exclusive Gating：在LSTM的门机制中，绝大多数门的值为0或者1，几乎很难落到0.5附近。当 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow0" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 时，残差块变成只有直接映射组成，阻碍卷积部分特征的传播；当 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 时，直接映射失效，退化为普通的卷积网络；&lt;/li&gt;
&lt;li&gt;Short-cut only gating： &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow0" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 时，此时网络便是[1]提出的直接映射的残差网络； &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=g%28x%29%5Crightarrow1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 时，退化为普通卷积网络；&lt;/li&gt;
&lt;li&gt;Dropout：类似于将直接映射乘以 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=1-p" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; ，所以会影响梯度的反向传播；&lt;/li&gt;
&lt;li&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=1%5Ctimes1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; conv： &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=1%5Ctimes1" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 卷积比直接映射拥有更强的表示能力，但是实验效果却不如直接映射，说明该问题更可能是优化问题而非模型容量问题。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;所以我们可以得出结论：假设1成立，即&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=y_l+%3D+x_l+%2B+%5Cmathcal%7BF%7D%28x_l%2C+w_l%29+%5Ctag%7B11%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=y_%7Bl%2B1%7D+%3D+x_%7Bl%2B1%7D+%2B+%5Cmathcal%7BF%7D%28x_%7Bl%2B1%7D%2C+w_%7Bl%2B1%7D%29+%3D+f%28y_l%29+%2B+%5Cmathcal%7BF%7D%28f%28y_l%29%2C+w_%7Bl%2B1%7D%29+%5Ctag%7B12%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;</content><link href="/Blog/archives/resnet/" rel="alternate"/><published>2021-07-22T00:00:00+08:06</published></entry><entry><id>/Blog/archives/schedule/</id><title>论文后续安排</title><updated>2021-07-28T09:20:31.648439+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;工作安排&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;首先为了让模型适用于UCF101和HDM51两个数据集上，搭建baseline并测试精度&lt;/p&gt;&lt;p&gt;之后尝试经典蒸馏方法并运用到上面查看突破口，想出创新点并应用在该领域&lt;/p&gt;&lt;p&gt;创新点：连接加上权重，将所有前面feature输入并加上权重&lt;/p&gt;</content><link href="/Blog/archives/schedule/" rel="alternate"/><published>2021-07-23T00:00:00+08:06</published></entry><entry><id>/Blog/archives/github%20HMDB51%E6%80%BB%E7%BB%93/</id><title>github阅读</title><updated>2021-07-28T09:20:31.648394+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;h2&gt;3D-CNN Method&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
  &lt;th&gt;&lt;/th&gt;
  &lt;th&gt;&lt;/th&gt;
  &lt;th&gt;&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
  &lt;td&gt;iDT&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;LRCN&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;CVPR 2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;LSTM composite model&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;C3D&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;2015&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;TSN&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;ECCV 2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;R3DCNN&lt;/td&gt;
  &lt;td&gt;NVIDIA&lt;/td&gt;
  &lt;td&gt;2016&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;P3D&lt;/td&gt;
  &lt;td&gt;MSRA&lt;/td&gt;
  &lt;td&gt;ICCV 2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;R3D/2.5D&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;T3D&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;2017&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
  &lt;td&gt;R2+1D&lt;/td&gt;
  &lt;td&gt;&lt;/td&gt;
  &lt;td&gt;2018&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;!-- more --&gt;
&lt;p&gt;--&lt;/p&gt;&lt;hr /&gt;
&lt;p&gt;General LIb:&lt;/p&gt;&lt;p&gt;[ video model zoo (caffe2) ] &lt;a href="https://github.com/facebookresearch/VMZ"&gt;https://github.com/facebookresearch/VMZ&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Currently, this codebase supports the following models:&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;R(2+1)D, MCx models &lt;a href="https://research.fb.com/wp-content/uploads/2018/04/a-closer-look-at-spatiotemporal-convolutions-for-action-recognition.pdf"&gt;[1]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;CSN models &lt;a href="https://arxiv.org/pdf/1904.02811.pdf"&gt;[2]&lt;/a&gt;.&lt;/li&gt;
&lt;li&gt;R(2+1)D and CSN models pre-trained on large-scale (65 million!) weakly-supervised public Instagram videos (&lt;strong&gt;IG-65M&lt;/strong&gt;) &lt;a href="https://research.fb.com/wp-content/uploads/2019/05/Large-scale-weakly-supervised-pre-training-for-video-action-recognition.pdf"&gt;[3]&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3&gt;C3D&lt;/h3&gt;
&lt;p&gt;[github caffe ]&lt;a href="https://github.com/facebook/C3D"&gt;https://github.com/facebook/C3D&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[ github tensorflow ]&lt;a href="https://github.com/hx173149/C3D-tensorflow"&gt;https://github.com/hx173149/C3D-tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[github pytorch] &lt;a href="https://github.com/jfzhang95/pytorch-video-recognition"&gt;https://github.com/jfzhang95/pytorch-video-recognition&lt;/a&gt;&lt;/p&gt;&lt;p&gt;3x3x3  Kernel&lt;/p&gt;&lt;figure style="flex: 262.77372262773724" &gt;&lt;img width="720" height="137" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/71853e0cfa11f4c2df712fc43d7ee2f4.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure style="flex: 42.767857142857146" &gt;&lt;img width="479" height="560" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/b43d027f12bba9f3525b3e699fd0ac8f.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;P3D&lt;/h3&gt;
&lt;p&gt;[ caffe ] &lt;a href="https://github.com/ZhaofanQiu/pseudo-3d-residual-networks"&gt;https://github.com/ZhaofanQiu/pseudo-3d-residual-networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[ pytorch ] &lt;a href="https://github.com/jfzhang95/pytorch-video-recognition"&gt;https://github.com/jfzhang95/pytorch-video-recognition&lt;/a&gt;&lt;/p&gt;&lt;p&gt;Learning spatio-temporal representation with pseudo-3d residual networks. In ICCV, 2017.&lt;/p&gt;&lt;figure style="flex: 79.02208201892745" &gt;&lt;img width="501" height="317" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/424604edeaa910b2e7f9068987cbd376.png" /&gt;&lt;figcaption&gt;1567834917557&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure style="flex: 121.82857142857142" &gt;&lt;img width="2132" height="875" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/2843da38780038aff460bbcd558bab05.png" /&gt;&lt;figcaption&gt;1567834970578&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;T3D*&lt;/h3&gt;
&lt;p&gt;Architecture: DenseNet + 3D&lt;/p&gt;&lt;p&gt;[ github pytorch] &lt;a href="https://github.com/MohsenFayyaz89/T3D"&gt;https://github.com/MohsenFayyaz89/T3D&lt;/a&gt;&lt;/p&gt;&lt;figure style="flex: 97.0059880239521" &gt;&lt;img width="648" height="334" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/8498c4f96d09598e772ad76adb771aa4.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure style="flex: 44.662921348314605" &gt;&lt;img width="477" height="534" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/c03698d9b35355fcf748ec0b723ba4bd.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Res3D/R3D&lt;/h3&gt;
&lt;p&gt;architecture:	ResNet + 3DConv&lt;/p&gt;&lt;p&gt;[github pytorch] &lt;a href="https://github.com/jfzhang95/pytorch-video-recognition"&gt;https://github.com/jfzhang95/pytorch-video-recognition&lt;/a&gt;&lt;/p&gt;&lt;figure style="flex: 59.01856763925729" &gt;&lt;img width="445" height="377" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/8150945c2e765ba0c3418c83799ad304.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure style="flex: 51.63170163170163" &gt;&lt;img width="443" height="429" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/023a150c62aa2f2fabe6010dca6c74b5.jpg" /&gt;&lt;figcaption&gt;img&lt;/figcaption&gt;&lt;/figure&gt;&lt;h4&gt;R2.5D&lt;/h4&gt;
&lt;figure style="flex: 59.58083832335329" &gt;&lt;img width="398" height="334" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/aa929bfe9c21b425a5c3ba07516d40c3.png" /&gt;&lt;figcaption&gt;1567836996218&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;R2+1D&lt;/h3&gt;
&lt;p&gt;[ offical video model zoo (caffe2) ] &lt;a href="https://github.com/facebookresearch/VMZ"&gt;https://github.com/facebookresearch/VMZ&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[ github PyTorch] &lt;a href="https://github.com/leftthomas/R2Plus1D-C3D"&gt;https://github.com/leftthomas/R2Plus1D-C3D&lt;/a&gt;&lt;/p&gt;&lt;figure style="flex: 49.527186761229316" &gt;&lt;img width="419" height="423" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/d9b208e0c8fd89040655af70eee8f95c.png" /&gt;&lt;figcaption&gt;Figure 2&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;R3DCNN&lt;/h3&gt;
&lt;p&gt;[NVIDIA]&lt;a href="https://research.nvidia.com/sites/default/files/publications/NVIDIA_R3DCNN_cvpr2016.pdf"&gt;https://research.nvidia.com/sites/default/files/publications/NVIDIA_R3DCNN_cvpr2016.pdf&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[tensorflow ]&lt;a href="https://github.com/breadbread1984/R3DCNN"&gt;https://github.com/breadbread1984/R3DCNN&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[tensorflow ] &lt;a href="https://github.com/kilsenp/R3DCNN-tensorflow"&gt;https://github.com/kilsenp/R3DCNN-tensorflow&lt;/a&gt;&lt;/p&gt;&lt;p&gt;architecture: C3D + RNN&lt;/p&gt;&lt;h3&gt;TSN&lt;/h3&gt;
&lt;p&gt;[github caffe ] &lt;a href="https://github.com/yjxiong/temporal-segment-networks"&gt;https://github.com/yjxiong/temporal-segment-networks&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[ caffe opensource ] &lt;a href="https://github.com/yjxiong/caffe"&gt;https://github.com/yjxiong/caffe&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[Paper] &lt;a href="https://arxiv.org/pdf/1608.00859.pdf"&gt;https://arxiv.org/pdf/1608.00859.pdf&lt;/a&gt;&lt;/p&gt;&lt;figure style="flex: 87.5" &gt;&lt;img width="1393" height="796" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/03c5b5d0c76ff8ac3419831b8c213d31.png" /&gt;&lt;figcaption&gt;1568038064174&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;I3D&lt;/h3&gt;
&lt;p&gt;Architecture: Inception base&lt;/p&gt;&lt;p&gt;[git keras ] &lt;a href="https://github.com/OanaIgnat/i3d_keras"&gt;https://github.com/OanaIgnat/i3d_keras&lt;/a&gt;&lt;/p&gt;&lt;p&gt;[Paper ] &lt;a href="https://arxiv.org/pdf/1705.07750.pdf"&gt;https://arxiv.org/pdf/1705.07750.pdf&lt;/a&gt;&lt;/p&gt;&lt;figure style="flex: 118.86792452830188" &gt;&lt;img width="2016" height="848" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/69f472dba62bd4c9c900fd5fad1f8aeb.png" /&gt;&lt;figcaption&gt;1568037268584&lt;/figcaption&gt;&lt;/figure&gt;&lt;figure style="flex: 97.72946859903382" &gt;&lt;img width="2023" height="1035" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/45c870c31d512a5092a0aff42829b21c.png" /&gt;&lt;figcaption&gt;1568037351528&lt;/figcaption&gt;&lt;/figure&gt;</content><link href="/Blog/archives/github%20HMDB51%E6%80%BB%E7%BB%93/" rel="alternate"/><published>2021-07-27T00:00:00+08:06</published></entry><entry><id>/Blog/archives/HMDB51%E5%A4%84%E7%90%86/</id><title>HMDB数据集</title><updated>2021-07-28T09:20:31.648322+00:00</updated><author><name>刘胜琪</name><email>1959376918@qq.com</email><uri>https://lsqsjtu.github.io/Blog/</uri></author><content>&lt;p&gt;&lt;strong&gt;HMDB51数据集&lt;/strong&gt;&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;orgin HMDB51-About 2GB for a total of 7,000 clips distributed in 51 action classes。&lt;/li&gt;
&lt;li&gt;&lt;a href="http://serre-lab.clps.brown.edu/wp-content/uploads/2013/10/hmdb51_sta.rar"&gt;Stabilized HMDB51&lt;/a&gt; – the number of clips and classes are the same as HMDB51, but there is a mask in [video_name].form associated with each clip. The mask file is readable in matlab.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;除了动作标签，还有meta-label（元标签）表征片段的属性&lt;/p&gt;&lt;p&gt;各个方向的，前面，后面，左右方向的视频&lt;/p&gt;&lt;p&gt;There should be 70 videos with id 1 , 30 videos with id 2 in each txt file.&lt;/p&gt;&lt;p&gt;mask:ice_cream:：只有为1的才表征人&lt;/p&gt;&lt;p&gt;stabilized：还有matrxi表征数据集对原始图像的转换&lt;/p&gt;&lt;p&gt;accimage: accelerate image对PIL库的部分功能实现&lt;/p&gt;&lt;p&gt;hog，hof特征为处理后的光流信息特征&lt;/p&gt;&lt;p&gt;&lt;a href="https://www.cnblogs.com/ocean1100/p/9494640.html"&gt;PyTorch载入图片后ToTensor解读（含PIL和OpenCV读取图片对比） - 木易修 - 博客园 (cnblogs.com)&lt;/a&gt;&lt;/p&gt;&lt;p&gt;totensor对于Opencv（不会除以255）和PIL.image的处理不一样&lt;/p&gt;&lt;p&gt;尝试&lt;a href="https://github.com/sebastiantiesmeyer/deeplabchop3d"&gt;GitHub - sebastiantiesmeyer/deeplabchop3d: inflated labchop kinetic net&lt;/a&gt;&lt;/p&gt;&lt;p&gt;直接复现最高精度的结果&lt;/p&gt;</content><link href="/Blog/archives/HMDB51%E5%A4%84%E7%90%86/" rel="alternate"/><published>2021-07-28T00:00:00+08:06</published></entry></feed>