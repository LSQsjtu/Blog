<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>我的个人博客</title><link>/Blog/</link><description>记录生活美好</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/f-logo.png</url><title>我的个人博客</title><link>/Blog/</link></image><language>zh-CN</language><lastBuildDate>Sat, 16 Jul 2022 11:56:30 +0806</lastBuildDate><pubDate>Sat, 16 Jul 2022 11:56:30 +0806</pubDate><item><title>翻墙实现</title><link>/Blog/archives/%E7%BF%BB%E5%A2%99%E5%AE%9E%E7%8E%B0/</link><description>&lt;p&gt;&lt;strong&gt;翻墙实现&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;虚拟私人网络：构建自己的局域网，无法有大公司，国家安全性保障&lt;/li&gt;
&lt;li&gt;多种协议，本地加密到远程服务器解密，流量通过远程服务器转发&lt;/li&gt;
&lt;li&gt;硬件实现，在路由器上部署翻墙软件代理，但是解密占用算力，导致性能下降，带宽下降。所以有的采用软路由，比路由器cpu算力高，但价格贵，有的采用老旧电脑实现。可以通过clash实现局域网关代理，实现所有设备翻墙&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实现方式：修改手机连接局域网代理&lt;/p&gt;&lt;img src="2022-05-20-翻墙实现.assets/image-20220521001134469.png" alt="image-20220521001134469" style="zoom:50%;" /&gt;
&lt;p&gt;获取节点方式&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;网络搜索节点池：“目前抓取源”&lt;/li&gt;
&lt;li&gt;GitHub搜索clash.yaml中关键词获取最新更新的clash.yaml&lt;/li&gt;
&lt;/ol&gt;
</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/%E7%BF%BB%E5%A2%99%E5%AE%9E%E7%8E%B0/</guid><pubDate>Fri, 20 May 2022 00:00:00 +0806</pubDate></item><item><title>PR学习</title><link>/Blog/archives/PR%E5%AD%A6%E4%B9%A0/</link><description>&lt;p&gt;&lt;strong&gt;PR 教程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;导出设置，调整目标比特率&lt;/p&gt;&lt;p&gt;![image-20220524220508293](PR 教程.assets/image-20220524220508293-16534011104501.png)&lt;/p&gt;&lt;p&gt;字幕是引用，所以每次都新建一个新的字幕，选择New Time Romin字体，居中……&lt;/p&gt;&lt;p&gt;视频直接拖动，音频拖动需要先解除链接，每个视频在不同位置，调整缩放是不需要选择帧这一项&lt;/p&gt;&lt;p&gt;图片可以直接拉动改变时长&lt;/p&gt;&lt;p&gt;![image-20220524223821001](PR 教程.assets/image-20220524223821001-16534031026721.png)&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/PR%E5%AD%A6%E4%B9%A0/</guid><pubDate>Tue, 24 May 2022 00:00:00 +0806</pubDate></item><item><title>shining</title><link>/Blog/archives/shining/</link><description>&lt;p&gt;&lt;strong&gt;闪灵&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;建立在印第安人坟地上的诅咒。当人自我走向罪恶的时候，不顾一切地去摧毁阻碍的东西，通过bgm渲染恐怖的氛围，将幽闭恐惧和超自然力量shining结合在一起，并不会让人感觉非常的害怕惊悚，没有突入其来的虐杀和恐怖，更多是通过演员的表现力传递出令人压迫的恐惧。&lt;/p&gt;&lt;p&gt;镜头调度非常好，但是由于打光的影响导致酒店所有房间都打开了灯，无法营造黑暗局促，压抑的氛围。通过揣度人物内心的黑暗面，细思极恐。&lt;/p&gt;</description><author>1959376918@qq.com (lsq)</author><guid isPermaLink="true">/Blog/archives/shining/</guid><pubDate>Sat, 11 Jun 2022 00:00:00 +0806</pubDate></item><item><title>保研准备</title><link>/Blog/archives/%E4%BF%9D%E7%A0%94%E5%87%86%E5%A4%87/</link><description>&lt;ul&gt;
&lt;li&gt;[ ] 熟悉c++&lt;/li&gt;
&lt;li&gt;[ ] 力扣刷题&lt;/li&gt;
&lt;li&gt;[ ] 英语自我介绍&lt;/li&gt;
&lt;li&gt;[ ] 论文概括，英文视频&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;老师关注的是安卓开发，MMMOT框架跟踪&lt;/p&gt;&lt;p&gt;线段树修改分为单点修改和区间修改：&lt;/p&gt;&lt;p&gt;区间修改使用到lazy标志，当需要查询节点下的值，需要将lazy更新到子树中，lazy=INF表示已经更新过了。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/%E4%BF%9D%E7%A0%94%E5%87%86%E5%A4%87/</guid><pubDate>Tue, 14 Jun 2022 00:00:00 +0806</pubDate></item><item><title>kernel approximation</title><link>/Blog/archives/kernel%20approximation/</link><description>&lt;p&gt;&lt;strong&gt;kernel approximation&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;用random tensor逼近kernel matrix&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;如何使用random feature去训练数据&lt;/li&gt;
&lt;li&gt;多少random feature需要去确保质量&lt;/li&gt;
&lt;li&gt;evaluation of popular random features based algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;data dependent（根据数据distribution采样）和data-independent（高斯分布，高斯采样就可以）&lt;/p&gt;&lt;p&gt;sample number bigger than dim but much smaller than n.&lt;/p&gt;&lt;p&gt;random feature model可以看作两层神经网络（固定参数）&lt;/p&gt;&lt;figure style="flex: 123.66071428571429" &gt;&lt;img width="554" height="224" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/0a5eb3adf2a8b1653a61a19b83727cf9.jpg" /&gt;&lt;figcaption&gt;double descent phenomena&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其中左边的部分即为传统统计学习中的bias-variance trade-off，此时模型的参数小于interpolation threshold，实际上这个threshold即为模型数与样本数相等的点（在多分类的情况下略有变化）。当模型的参数超过这个threshold的时候，模型通过进一步增加参数以增加可能的最小范数模型的范围、进而达到降低test error。考虑到double descent现象是从最近的统计学习方法中发现的，与我们经典统计学习中的“U型”特征不同，故称上图左侧为classical regime，此时模型的状态是under-parameterized；而称上图右侧为modern interpolating regime，此时模型的状态是over-parameterized。&lt;/p&gt;&lt;p&gt;kernel learning by random features是通过学习分布得到。先学习random feature&lt;/p&gt;&lt;p&gt;通过算法可以得到random feature数量级单位&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/kernel%20approximation/</guid><pubDate>Sat, 18 Jun 2022 00:00:00 +0806</pubDate></item><item><title>neural tangent kernel (NTK)</title><link>/Blog/archives/neural%20tangent%20kernel%20%28NTK%29/</link><description>&lt;h3&gt;neural tangent kernel (NTK)&lt;/h3&gt;
&lt;h4&gt;观点：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;NTK在无限宽极限下趋于一个确定的核(Kernel)，在无限宽极限下不随着时间变化==（是训练中的不变量）==lazy training，but not always the case，说明在参数空间中变化很小，所以可以用一阶泰勒近似，只是linear in w，对w求导，not linear in x（有非线性激活函数）。而且在梯度下降的训练过程中保持不变，因此==无限宽网络==的输出层结果的动力学可以用一个常微分方程来表示。可以控制梯度，有上下限。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;无限宽的神经网络等价于高斯过程，相当于一个线性过程，可以线性叠加并且可以在不同初始条件收敛到同一过程&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean field Theory中存在超参数平面中的一条临界线&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;figure style="flex: 195.3804347826087" &gt;&lt;img width="719" height="184" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/3dd0f1c10fc0e2b9c38a15962b662f01.png" /&gt;&lt;figcaption&gt;image-20220620212719766&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;==引入NTK parameterziation==，是因为LeCun参数化会导致NTK在无限宽极限下发散。神经网络是不凸的，有很多局部最优点，通过NTK可以通过梯度下降到全局最优点&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220623165503742.png" alt="image-20220623165503742" style="zoom:50%;" /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK就是前h层梯度积，经验NTK在宽度趋向无穷时可以趋近理想NTK，NTK只和网络层数有关，网络层数&amp;gt;=2  ，这种无穷NTK比多项式更好&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK是低维的，低维的特性（数据），skip connection&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;提出的问题困难：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;NTK在有限宽网络会失效：&lt;strong&gt;有限宽度到无穷的NTK演变问题&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;kernel trick：kernel将低维映射到高维，容易造成数据复杂度上升，难以采样，但常常是不考虑值而是考虑两个输出的内积。用kernel matrix去描述，对称阵，且是半正定的，这样可以节省空间和计算开销&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;数学知识：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220622163321364.png" alt="image-20220622163321364" style="zoom:67%;" /&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220627225110376.png" alt="关键公式" style="zoom:50%;" /&gt;&lt;/p&gt;&lt;p&gt;其中第一个等式来自第二个性质，第二个等式来自第一个性质，而 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5CTheta_%5Cinfty%28X%2CX%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 就是那位deterministic的Kerenl。&lt;/p&gt;&lt;p&gt;于是网络输出函数的动力学方程变为：&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_t%28%5Ctheta%2C+x%29%7D%7B%5Cpartial+t%7D+%3D++-+%5Ceta+%5CTheta_%5Cinfty%28x%2CX%29+%5Cnabla_%7Bf_t%28%5Ctheta%2CX%29%7D+%5Cmathcal%7BL%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;假设loss function是我们常见的Mean squared loss，那么方程进一步化简为一个线性常微分方程（ODE）：&lt;/p&gt;&lt;figure style="flex: 539.5348837209302" &gt;&lt;img width="464" height="43" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/bc6c7f594d98f5ef62651fcc3b5b1950.png" /&gt;&lt;figcaption&gt;image-20220622163732369&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当将函数一阶泰勒展开并保留一阶时，和上面ode等价，所以可以认为是一阶线性模型。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;名字由来tangent是因为取的是一阶泰勒导数&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK当宽度到($\frac{n}{d}=\Omega(d^{l-1})$）时可以近似到$+\infin$宽度&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;模型结构：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;何凯明initialization添加系数$\sqrt[2]{\frac{c_\sigma}{d_h}}$，其中$d_h$为h层神经元个数，这样保证每一层范数相等&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;研究方向：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;研究NTK的trainability and generalization， the NNGP (randomly initialized neural networks with certain class) kernel K and the tangent kernel Θ&lt;/li&gt;
&lt;/ol&gt;
</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/neural%20tangent%20kernel%20%28NTK%29/</guid><pubDate>Mon, 20 Jun 2022 00:00:00 +0806</pubDate></item><item><title>唐人街探案</title><link>/Blog/archives/chinatown/</link><description>&lt;p&gt;&lt;strong&gt;唐人街探案&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每个人内心都有善恶，每个人都有内心的不如意，但是每个人都要勇敢得去面对。&lt;/p&gt;&lt;p&gt;横移镜头有荒诞的喜剧效果，重复夸张的摔跤总是让人发笑，节奏把控的很好。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/chinatown/</guid><pubDate>Mon, 20 Jun 2022 00:00:00 +0806</pubDate></item><item><title>普罗旺斯的夏天</title><link>/Blog/archives/the%20relax%20summer/</link><description>&lt;p&gt;&lt;strong&gt;普罗旺斯的夏天&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每个地方都有一个宁静而又祥和的一天，在普罗旺斯，能在早上看到天微微泛红，渐渐地展露新的一天，展现在人们面前最纯真的大自然。&lt;/p&gt;&lt;p&gt;第一天因为机考而睡不着觉，煎熬得度过这漫长的一天，渐渐地忘记睡觉，但是也是能够开始好好整理一下自己的生活，准备好前往下一个阶段的人生。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/the%20relax%20summer/</guid><pubDate>Sun, 10 Jul 2022 00:00:00 +0806</pubDate></item><item><title>Paper Writing</title><link>/Blog/archives/Paper%20Writing/</link><description>&lt;p&gt;&lt;strong&gt;the craft of research&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;第三部分&lt;/p&gt;&lt;p&gt;how to make a claim？&lt;/p&gt;&lt;p&gt;how to let reader believe you？&lt;/p&gt;&lt;p&gt;尝试通过对话提前解决读者疑惑，说明工作的局限性&lt;/p&gt;&lt;p&gt;为了让故事更饱满，在边做的时候边写，让故事逻辑更丰满，让读者觉得你做的工作更有意义。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/Paper%20Writing/</guid><pubDate>Fri, 15 Jul 2022 00:00:00 +0806</pubDate></item><item><title>人生感受</title><link>/Blog/archives/lifefeeling/</link><description>&lt;p&gt;&lt;strong&gt;人生感受&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;如果平常工作只是考虑表面，没有考虑自己是为了什么，如何实现自己的人生理想，最后也常常会因为不经意之间的鸡毛蒜皮感受到沮丧&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/lifefeeling/</guid><pubDate>Sat, 16 Jul 2022 00:00:00 +0806</pubDate></item></channel></rss>