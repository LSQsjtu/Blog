<?xml version='1.0' encoding='UTF-8'?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" version="2.0"><channel><title>我的个人博客</title><link>/Blog/</link><description>记录生活美好</description><docs>http://www.rssboard.org/rss-specification</docs><generator>python-feedgen</generator><image><url>https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/f-logo.png</url><title>我的个人博客</title><link>/Blog/</link></image><language>zh-CN</language><lastBuildDate>Wed, 29 Jun 2022 06:31:12 +0806</lastBuildDate><pubDate>Wed, 29 Jun 2022 06:31:12 +0806</pubDate><item><title>leetcode刷题</title><link>/Blog/archives/c%2B%2B%20part/</link><description>&lt;p&gt;&lt;strong&gt;C++&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;生成格式vector&lt;int&gt; 变量名&lt;/p&gt;&lt;p&gt;vector.push_back()放入最后，&lt;/p&gt;&lt;p&gt;queue&amp;lt;类型&amp;gt; 变量名 queue.push(), queue.pop(), 先进先出&lt;/p&gt;&lt;p&gt;move(vector) allowing the efficient transfer of resources from &lt;code&gt;t&lt;/code&gt; to another object. &lt;strong&gt;唯一的功能是把左值强制转化为右值&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;&lt;strong&gt;实现移动语义，避免拷贝，从而提升程序性能&lt;/strong&gt;, 这样可以避免深拷贝，只提取数值。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/c%2B%2B%20part/</guid><pubDate>Fri, 08 Apr 2022 00:00:00 +0806</pubDate></item><item><title>Paper Reading</title><link>/Blog/archives/Paper%20Reading/</link><description>&lt;p&gt;&lt;strong&gt;阅读论文&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;1.title
2.abstract
3.introduction
4.method
5.experiments
6.conclusion&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;第一遍：==标题、摘要、结论==。可以看一看==方法和实验部分重要的图和表==。这样可以花费十几分钟时间了解到论文是否适合你的研究方向。&lt;/li&gt;
&lt;li&gt;第二遍：确定论文值得读之后，可以快速的把整个论文过一遍，不需要知道所有的细节，需要==了解重要的图和表==，知道每一个部分在干什么，作者相比与其他人提出了什么==不同的方法==，圈出==相关文献==。觉得文章太难，可以读==引用的文献==。&lt;/li&gt;
&lt;li&gt;第三遍：提出什么==问题==，用什么==方法==来解决这个问题。==实验==是怎么做的。合上文章，回忆每一个部分在讲什么。换我来可以怎么做，==能不能做得更好==。&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;论文项目三步分解法&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;问题&lt;/li&gt;
&lt;li&gt;方法&lt;/li&gt;
&lt;li&gt;结果，以及未来畅想（将某些部分替换打补丁）&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;strong&gt;经典论文&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;==Resnet==&lt;/p&gt;&lt;p&gt;残差连接：学习浅层网络没有学习到的residual，当输入为0时后面的网络层基本为0，没有多余的信息，学习==$H(x) - x$==的信息。网络结构的设计使得梯度稳定，不会到零（后人的解释）。原始想法是将机器学习中学习残差的思想运用到深度网络训练中。&lt;/p&gt;&lt;p&gt;bottleneck：先降维，然后提取信息，再提升维度，起到降低算法复杂度的功能，不用学习无用维度的信息。&lt;/p&gt;&lt;p&gt;==Generative adversarial nets==&lt;/p&gt;&lt;p&gt;写作方面&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;对于开创性的工作来说主要说明自己独一无二的地方，基于其他方法改进的主要突出强调区别&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;创新点&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;以往是近似构造似然函数去学习这个分布，现在通过模型网络来直接&lt;strong&gt;学习分布&lt;/strong&gt;（于是可以训练了），分布的求解往往是不可逆的，无法求出分布的逆函数，来通过参数求出对应的输出数据，于是采用非常大的数据去盗版模仿&lt;/li&gt;
&lt;li&gt;通过G和D去对抗，minmax最优化分布函数，求取最优值&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;新的知识点&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;==KL散度==：如果D过拟合会导致输出为0，不好优化求导，后面有工作去更新优化方式，通过更简约的方法去学习这个分布。&lt;/li&gt;
&lt;li&gt;如果数据不属于同一个分布，那么就可以训练同一个二分类去分类&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&lt;strong&gt;BERT&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;芝麻街中人物姓名&lt;/p&gt;&lt;p&gt;解决问题&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;可以预训练后广泛运用在其他任务上（加一个输出层）&lt;/li&gt;
&lt;li&gt;将预训练过程和大模型过程&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;创新点&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;引入==mask==，完形填空，从而可以利用双向的信息，而不是单纯地就将从左向右和从右向左结合在一起&lt;/li&gt;
&lt;li&gt;证明了利用大量没有标号的数据训练，比利用少量有标号的数据训练要更好&lt;/li&gt;
&lt;li&gt;任务相对简单，故&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写作&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;数值展示：绝对值（相对值）&lt;/li&gt;
&lt;li&gt;比较，开门见山，比较BERT和GPT，Elmo&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;GPT（在下游任务上微调，分阶段训练）&lt;/p&gt;&lt;p&gt;ELmo: RNN 提取特征后再一起用于训练&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;MAE: masked autoencoder&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;解决问题：将Bert引入cv中存在三个问题&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;位置信息不好和图片信息一起输入transform中（但是一般CNN网络架构中顺序卷积会将位置信息传递下去），==ViT已经解决==。&lt;/li&gt;
&lt;li&gt;信息密度不一样（词和图片），词不好去掉，但是图片冗余信息多，所以要丢掉高比率随机块（去掉很多块）、&lt;/li&gt;
&lt;li&gt;decoder的设计，不同层次信息&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;创新点：&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;将ViT和Bert结合起来，构建非对称的autoencoder（encoder只关注没有mask的部分，decoder关注所有信息，并且输入对应的位置信息，重新==生成==mask的部分）&lt;/li&gt;
&lt;li&gt;发现盖住75%的图片，random掩码能够得到较好的信息（示例图片amazing，但是对于其他图片或许没有这么好的效果）&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知识点：&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;ViT：将图片分割成16$\times$16的分块，当做词语来学习&lt;/li&gt;
&lt;li&gt;auto：x，y来自同一个样本（如同一个句子或同样的图片），快的模型：efficient，大的模型：scalable&lt;/li&gt;
&lt;li&gt;字词是高层次语义信息，所以简单输出层encoder就可以了，图片语义分割常采用转置的神经网络解码器&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写作：&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;名字：&lt;strong&gt;什么是什么&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;写清训练和测试过程中的操作安排，讲好自己为什么这样做，并且有数据支撑&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;MoCo&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;Momentum Contrast learning&lt;/p&gt;&lt;p&gt;解决问题：&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;无监督学习无法泛化，相比较于nlp领域中是离散的key（容易建模），CV中图片信息冗余，需要建立一个更加大的字典一一对应&lt;/li&gt;
&lt;li&gt;解决动态字典内容少，信息不一致的情况&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;创新点&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;动量对比学习 $\theta_{k+1} = m\theta_{k}+\theta_{q}$，m设置为0.999，此时更新k的部分慢&lt;/li&gt;
&lt;li&gt;用FIFO队列维护字典，每次出去mini_batch的数据，相比较于end-to-end（大小受限），memory bank（not consistent）方法更有效&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;知识&lt;/p&gt;&lt;ul&gt;
&lt;li&gt;对比学习，pretext task（前置任务）区分正负样本，让正样本更接近，负样本分布更离散。最简单的instance discrimination将图片随机裁剪，旋转生成新的图片，视为正样本，其余视为负样本&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;写作&lt;/p&gt;&lt;blockquote&gt;
&lt;p&gt;说明为什么要使用这个method&lt;/p&gt;&lt;p&gt;论文&lt;/p&gt;&lt;p&gt;introduction强调high level的cage的作用&lt;/p&gt;&lt;p&gt;文章公式罗列：说明为什么要这样使用&lt;/p&gt;&lt;/blockquote&gt;
&lt;p&gt;&lt;strong&gt;DeepMind 用机器学习指导视觉&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;想法：通过机器学习观察$X(Z),Y(z)$之间映射函数之间的关系，通过梯度大小反应对应量是否会影响最终目标值。&lt;/p&gt;&lt;p&gt;使用automl类工具，让你更加专注于问题本身，例如autogluon&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/Paper%20Reading/</guid><pubDate>Fri, 15 Apr 2022 00:00:00 +0806</pubDate></item><item><title>ffmpeg教程</title><link>/Blog/archives/ffmpeg%E6%95%99%E7%A8%8B/</link><description>&lt;h1&gt;ffmpeg教程&lt;/h1&gt;
&lt;h2&gt;在Linux上安装&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;如果没有更高的权限，可以这么安装&lt;ul&gt;
&lt;li&gt;将那个&lt;code&gt;.tar.xz&lt;/code&gt;文件解压&lt;/li&gt;
&lt;li&gt;&lt;code&gt;xz -d ***.xz&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;tar -xvf ***.tar&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;然后有一个ffmpeg就可以直接用了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;常用指令&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;查看视频的基本信息&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i input.mp4&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;裁剪&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i input.mp4 -vf crop=&amp;quot;720:720:280:0&amp;quot; output.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;vf的意思是video filter&lt;/li&gt;
&lt;li&gt;crop里面的四个参数分别是whxy(width, height, 从x开始裁剪, 从y开始裁剪)&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;调整帧率&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i input -r 25 output&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;-r后面输入帧率就可以了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;resize video&lt;ul&gt;
&lt;li&gt;假设input是1920x1080，想变成480x320&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i input -vf scale=480:-1 output&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;如果是-1，就会按照原来的尺寸进行缩放&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;将视频帧导出&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i input -r 25/1 %6d.png&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;帧速率指定为25帧1秒&lt;/li&gt;
&lt;li&gt;%6d表示需要是6位&lt;/li&gt;
&lt;li&gt;%06d也可以&lt;/li&gt;
&lt;li&gt;输出png&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;图片合成视频&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i &amp;lt;input&amp;gt; -r &amp;lt;fps&amp;gt; &amp;lt;output&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;比如图片的名字都是&lt;code&gt;img000001.png&lt;/code&gt;这种形式，那么input就写&lt;code&gt;img%06d.png&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;可以添加&lt;code&gt;-y&lt;/code&gt;表示覆盖&lt;/li&gt;
&lt;li&gt;这样生成的视频是没有声音的&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;音视频合成&lt;ul&gt;
&lt;li&gt;适用于视频本身没有声音的情况&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i &amp;lt;audio&amp;gt; -i &amp;lt;video&amp;gt; &amp;lt;output&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;截取一段视频&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i &amp;lt;input&amp;gt; -ss 3 -t 23 &amp;lt;output&amp;gt;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;从3s开始，截取长度为23s的视频&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;合并音频(concat)&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i &amp;quot;concat:1.mp3|2.mp3|3.mp3&amp;quot; -acodec copy concat.mp3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;这个方法只支持MP3格式&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;混合音频(remix)&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i 1.mp3 -i 2.mp3 -i 3.mp3 -filter_complex amix=inputs=3:duration=first:dropout_transition=2 output.mp3&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;要是多个输入就改变&lt;code&gt;inputs=&amp;lt;num_input&amp;gt;&lt;/code&gt;就可以了&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;修改视频比特率&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -i input.mp4 -b:v 1000k output.mp4&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;合成视频&lt;ul&gt;
&lt;li&gt;&lt;code&gt;ffmpeg -f concat -safe 0 -i mylist.txt -c copy myvideo.mp4&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;-safe 0 防止出现“unsafe warning”&lt;/li&gt;
&lt;li&gt;txt格式 &lt;code&gt;file &amp;quot;file_path&amp;quot;&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;ffmpeg -r 30 -i folder_name/%06d.png -c:v libx264 -crf 0 output.mp4&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/ffmpeg%E6%95%99%E7%A8%8B/</guid><pubDate>Sat, 07 May 2022 00:00:00 +0806</pubDate></item><item><title>翻墙实现</title><link>/Blog/archives/%E7%BF%BB%E5%A2%99%E5%AE%9E%E7%8E%B0/</link><description>&lt;p&gt;&lt;strong&gt;翻墙实现&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;虚拟私人网络：构建自己的局域网，无法有大公司，国家安全性保障&lt;/li&gt;
&lt;li&gt;多种协议，本地加密到远程服务器解密，流量通过远程服务器转发&lt;/li&gt;
&lt;li&gt;硬件实现，在路由器上部署翻墙软件代理，但是解密占用算力，导致性能下降，带宽下降。所以有的采用软路由，比路由器cpu算力高，但价格贵，有的采用老旧电脑实现。可以通过clash实现局域网关代理，实现所有设备翻墙&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;实现方式：修改手机连接局域网代理&lt;/p&gt;&lt;img src="2022-05-20-翻墙实现.assets/image-20220521001134469.png" alt="image-20220521001134469" style="zoom:50%;" /&gt;
&lt;p&gt;获取节点方式&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;网络搜索节点池：“目前抓取源”&lt;/li&gt;
&lt;li&gt;GitHub搜索clash.yaml中关键词获取最新更新的clash.yaml&lt;/li&gt;
&lt;/ol&gt;
</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/%E7%BF%BB%E5%A2%99%E5%AE%9E%E7%8E%B0/</guid><pubDate>Fri, 20 May 2022 00:00:00 +0806</pubDate></item><item><title>PR学习</title><link>/Blog/archives/PR%E5%AD%A6%E4%B9%A0/</link><description>&lt;p&gt;&lt;strong&gt;PR 教程&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;导出设置，调整目标比特率&lt;/p&gt;&lt;p&gt;![image-20220524220508293](PR 教程.assets/image-20220524220508293-16534011104501.png)&lt;/p&gt;&lt;p&gt;字幕是引用，所以每次都新建一个新的字幕，选择New Time Romin字体，居中……&lt;/p&gt;&lt;p&gt;视频直接拖动，音频拖动需要先解除链接，每个视频在不同位置，调整缩放是不需要选择帧这一项&lt;/p&gt;&lt;p&gt;图片可以直接拉动改变时长&lt;/p&gt;&lt;p&gt;![image-20220524223821001](PR 教程.assets/image-20220524223821001-16534031026721.png)&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/PR%E5%AD%A6%E4%B9%A0/</guid><pubDate>Tue, 24 May 2022 00:00:00 +0806</pubDate></item><item><title>shining</title><link>/Blog/archives/shining/</link><description>&lt;p&gt;&lt;strong&gt;闪灵&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;建立在印第安人坟地上的诅咒。当人自我走向罪恶的时候，不顾一切地去摧毁阻碍的东西，通过bgm渲染恐怖的氛围，将幽闭恐惧和超自然力量shining结合在一起，并不会让人感觉非常的害怕惊悚，没有突入其来的虐杀和恐怖，更多是通过演员的表现力传递出令人压迫的恐惧。&lt;/p&gt;&lt;p&gt;镜头调度非常好，但是由于打光的影响导致酒店所有房间都打开了灯，无法营造黑暗局促，压抑的氛围。通过揣度人物内心的黑暗面，细思极恐。&lt;/p&gt;</description><author>1959376918@qq.com (lsq)</author><guid isPermaLink="true">/Blog/archives/shining/</guid><pubDate>Sat, 11 Jun 2022 00:00:00 +0806</pubDate></item><item><title>保研准备</title><link>/Blog/archives/%E4%BF%9D%E7%A0%94%E5%87%86%E5%A4%87/</link><description>&lt;ul&gt;
&lt;li&gt;[ ] 熟悉c++&lt;/li&gt;
&lt;li&gt;[ ] 力扣刷题&lt;/li&gt;
&lt;li&gt;[ ] 英语自我介绍&lt;/li&gt;
&lt;li&gt;[ ] 论文概括，英文视频&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;老师关注的是安卓开发，MMMOT框架跟踪&lt;/p&gt;&lt;p&gt;线段树修改分为单点修改和区间修改：&lt;/p&gt;&lt;p&gt;区间修改使用到lazy标志，当需要查询节点下的值，需要将lazy更新到子树中，lazy=INF表示已经更新过了。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/%E4%BF%9D%E7%A0%94%E5%87%86%E5%A4%87/</guid><pubDate>Tue, 14 Jun 2022 00:00:00 +0806</pubDate></item><item><title>kernel approximation</title><link>/Blog/archives/kernel%20approximation/</link><description>&lt;p&gt;&lt;strong&gt;kernel approximation&lt;/strong&gt;&lt;/p&gt;&lt;ol&gt;
&lt;li&gt;如何使用random feature去训练数据&lt;/li&gt;
&lt;li&gt;多少random feature需要去确保质量&lt;/li&gt;
&lt;li&gt;evaluation of popular random features based algorithms&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;data dependent（根据数据distribution采样）和data-independent（高斯分布，高斯采样就可以）&lt;/p&gt;&lt;p&gt;sample number bigger than dim but much smaller than n.&lt;/p&gt;&lt;p&gt;random feature model可以看作两层神经网络（固定参数）&lt;/p&gt;&lt;figure style="flex: 123.66071428571429" &gt;&lt;img width="554" height="224" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/0a5eb3adf2a8b1653a61a19b83727cf9.jpg" /&gt;&lt;figcaption&gt;double descent phenomena&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;其中左边的部分即为传统统计学习中的bias-variance trade-off，此时模型的参数小于interpolation threshold，实际上这个threshold即为模型数与样本数相等的点（在多分类的情况下略有变化）。当模型的参数超过这个threshold的时候，模型通过进一步增加参数以增加可能的最小范数模型的范围、进而达到降低test error。考虑到double descent现象是从最近的统计学习方法中发现的，与我们经典统计学习中的“U型”特征不同，故称上图左侧为classical regime，此时模型的状态是under-parameterized；而称上图右侧为modern interpolating regime，此时模型的状态是over-parameterized。&lt;/p&gt;&lt;p&gt;kernel learning by random features是通过学习分布得到。先学习random feature&lt;/p&gt;&lt;p&gt;通过算法可以得到random feature数量级单位&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/kernel%20approximation/</guid><pubDate>Sat, 18 Jun 2022 00:00:00 +0806</pubDate></item><item><title>唐人街探案</title><link>/Blog/archives/chinatown/</link><description>&lt;p&gt;&lt;strong&gt;唐人街探案&lt;/strong&gt;&lt;/p&gt;&lt;p&gt;每个人内心都有善恶，每个人都有内心的不如意，但是每个人都要勇敢得去面对。&lt;/p&gt;&lt;p&gt;横移镜头有荒诞的喜剧效果，重复夸张的摔跤总是让人发笑，节奏把控的很好。&lt;/p&gt;</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/chinatown/</guid><pubDate>Mon, 20 Jun 2022 00:00:00 +0806</pubDate></item><item><title>neural tangent kernel (NTK)</title><link>/Blog/archives/neural%20tangent%20kernel%20%28NTK%29/</link><description>&lt;h3&gt;neural tangent kernel (NTK)&lt;/h3&gt;
&lt;h4&gt;观点：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;NTK在无限宽极限下趋于一个确定的核(Kernel)，在无限宽极限下不随着时间变化==（是训练中的不变量）==lazy training，but not always the case，说明在参数空间中变化很小，所以可以用一阶泰勒近似，只是linear in w，对w求导，not linear in x（有非线性激活函数）。而且在梯度下降的训练过程中保持不变，因此==无限宽网络==的输出层结果的动力学可以用一个常微分方程来表示。可以控制梯度，有上下限。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;无限宽的神经网络等价于高斯过程，相当于一个线性过程，可以线性叠加并且可以在不同初始条件收敛到同一过程&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;Mean field Theory中存在超参数平面中的一条临界线&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;figure style="flex: 195.3804347826087" &gt;&lt;img width="719" height="184" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/3dd0f1c10fc0e2b9c38a15962b662f01.png" /&gt;&lt;figcaption&gt;image-20220620212719766&lt;/figcaption&gt;&lt;/figure&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;==引入NTK parameterziation==，是因为LeCun参数化会导致NTK在无限宽极限下发散。神经网络是不凸的，有很多局部最优点，通过NTK可以通过梯度下降到全局最优点&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220623165503742.png" alt="image-20220623165503742" style="zoom:50%;" /&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;NTK就是前h层梯度积，经验NTK在宽度趋向无穷时可以趋近理想NTK，NTK只和网络层数有关，网络层数&amp;gt;=2  ，这种无穷NTK比多项式更好&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;提出的问题困难：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;NTK在有限宽网络会失效：&lt;strong&gt;有限宽度到无穷的NTK演变问题&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;kernel trick：kernel将低维映射到高维，容易造成数据复杂度上升，难以采样，但常常是不考虑值而是考虑两个输出的内积。用kernel matrix去描述，对称阵，且是半正定的，这样可以节省空间和计算开销&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;数学知识：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;p&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220622163321364.png" alt="image-20220622163321364" style="zoom:67%;" /&gt;&lt;img src="2022-06-20-neural-tangent-kernel-(NTK).assets/image-20220627225110376.png" alt="关键公式" style="zoom:50%;" /&gt;&lt;/p&gt;&lt;p&gt;其中第一个等式来自第二个性质，第二个等式来自第一个性质，而 &lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5CTheta_%5Cinfty%28X%2CX%29" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt; 就是那位deterministic的Kerenl。&lt;/p&gt;&lt;p&gt;于是网络输出函数的动力学方程变为：&lt;/p&gt;&lt;figure  size-undefined&gt;&lt;img width="-1" height="-1" src="https://www.zhihu.com/equation?tex=%5Cfrac%7B%5Cpartial+f_t%28%5Ctheta%2C+x%29%7D%7B%5Cpartial+t%7D+%3D++-+%5Ceta+%5CTheta_%5Cinfty%28x%2CX%29+%5Cnabla_%7Bf_t%28%5Ctheta%2CX%29%7D+%5Cmathcal%7BL%7D" /&gt;&lt;figcaption&gt;[公式]&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;假设loss function是我们常见的Mean squared loss，那么方程进一步化简为一个线性常微分方程（ODE）：&lt;/p&gt;&lt;figure style="flex: 539.5348837209302" &gt;&lt;img width="464" height="43" src="https://cdn.jsdelivr.net/gh/LSQsjtu/Blog@gh-pages/archives/assets/bc6c7f594d98f5ef62651fcc3b5b1950.png" /&gt;&lt;figcaption&gt;image-20220622163732369&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;当将函数一阶泰勒展开并保留一阶时，和上面ode等价，所以可以认为是一阶线性模型。&lt;/p&gt;&lt;/li&gt;
&lt;li&gt;&lt;p&gt;名字由来tangent是因为取的是一阶泰勒导数&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;模型结构：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;何凯明initialization添加系数$\sqrt[2]{\frac{c_\sigma}{d_h}}$，其中$d_h$为h层神经元个数，这样保证每一层范数相等&lt;/li&gt;
&lt;/ol&gt;
&lt;h4&gt;研究方向：&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;研究NTK的trainability and generalization， the NNGP (randomly initialized neural networks with certain class) kernel K and the tangent kernel Θ&lt;/li&gt;
&lt;/ol&gt;
</description><author>1959376918@qq.com (刘胜琪)</author><guid isPermaLink="true">/Blog/archives/neural%20tangent%20kernel%20%28NTK%29/</guid><pubDate>Mon, 20 Jun 2022 00:00:00 +0806</pubDate></item></channel></rss>